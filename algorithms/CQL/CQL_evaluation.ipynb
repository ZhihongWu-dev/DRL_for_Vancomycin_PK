{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860a5fcc-cb84-4f2a-ab09-782e7c586379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CQL连续动作空间模型评估\n",
      "================================================================================\n",
      "\n",
      "1. 加载模型...\n",
      "✅ 模型已加载\n",
      "   状态维度: 7\n",
      "   动作维度: 1\n",
      "   配置: alpha=0.01, gamma=0.99\n",
      "\n",
      "2. 加载数据...\n",
      "✅ 数据已加载\n",
      "   总记录数: 2113\n",
      "   总episode数: 58\n",
      "   奖励范围（归一化）: [0.0000, 1.0000]\n",
      "\n",
      "3. 执行FQE评估...\n",
      "\n",
      "【CQL策略价值（FQE评估）】\n",
      "  策略期望价值: 63.6100 ± 4.8356\n",
      "  平均状态价值: 61.2058 ± 29.7328\n",
      "  评估episode数: 58\n",
      "\n",
      "4. 评估行为策略...\n",
      "\n",
      "【行为策略价值（实际数据）】\n",
      "  实际平均回报: 17.0907 ± 10.1630\n",
      "  评估episode数: 58\n",
      "\n",
      "5. 策略改进分析...\n",
      "\n",
      "【策略改进分析】\n",
      "  绝对改进: 46.5193\n",
      "  相对改进: 272.19%\n",
      "  解释: CQL策略比行为策略好 272.19%\n",
      "\n",
      "6. Episode级别详细分析...\n",
      "\n",
      "【Episode级别统计】\n",
      "  评估episode数: 58\n",
      "  平均episode长度: 36.4 步\n",
      "  平均初始价值: 63.6250\n",
      "  平均实际回报: 22.0597\n",
      "  平均折扣回报: 17.0907\n",
      "  平均动作MAE: 162.56 mg\n",
      "  平均策略动作: 69.78 mg\n",
      "  平均数据动作: 116.11 mg\n",
      "\n",
      "================================================================================\n",
      "评估总结\n",
      "================================================================================\n",
      "\n",
      "【策略价值（FQE评估）】\n",
      "  CQL策略期望价值: 63.6100\n",
      "  行为策略实际回报: 17.0907\n",
      "\n",
      "【策略改进】\n",
      "  272.19%\n",
      "  CQL策略比行为策略好 272.19%\n",
      "\n",
      "【关键发现】\n",
      "  • 策略价值是评估RL策略的核心指标\n",
      "  • 平均动作MAE: 162.56 mg\n",
      "  • 在医疗场景中，保守的策略（价值略低）可能是优点\n",
      "================================================================================\n",
      "\n",
      "✅ 评估完成！\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CQL连续动作空间模型评估脚本\n",
    "\n",
    "评估方法：\n",
    "1. FQE (Fitted Q Evaluation) - 策略价值评估\n",
    "2. 与行为策略对比\n",
    "3. Episode级别分析\n",
    "4. 可视化\n",
    "\n",
    "运行方式：\n",
    "    python evaluate_cql_continuous.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 模型定义（与训练脚本保持一致）\n",
    "def create_mlp(input_dim, output_dim, hidden_sizes=(256, 256), activation=nn.ReLU):\n",
    "    layers = []\n",
    "    last_dim = input_dim\n",
    "    for h in hidden_sizes:\n",
    "        layers.extend([nn.Linear(last_dim, h), activation()])\n",
    "        last_dim = h\n",
    "    layers.append(nn.Linear(last_dim, output_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.net = create_mlp(state_dim, 2 * action_dim, hidden_sizes)\n",
    "        self.log_std_min = -5\n",
    "        self.log_std_max = 2\n",
    "    def forward(self, state):\n",
    "        mean_logstd = self.net(state)\n",
    "        mean, log_std = torch.chunk(mean_logstd, 2, dim=-1)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = self.log_std_min + 0.5 * (log_std + 1) * (self.log_std_max - self.log_std_min)\n",
    "        std = torch.exp(log_std)\n",
    "        return mean, std\n",
    "    def sample(self, state):\n",
    "        mean, std = self(state)\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()\n",
    "        action = torch.tanh(x_t)\n",
    "        log_prob = normal.log_prob(x_t) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "        return action, log_prob, torch.tanh(mean)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.net = create_mlp(state_dim + action_dim, 1, hidden_sizes)\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=-1)\n",
    "        return self.net(sa)\n",
    "\n",
    "class CQLAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.q1 = QNetwork(state_dim, action_dim, hidden_sizes)\n",
    "        self.q2 = QNetwork(state_dim, action_dim, hidden_sizes)\n",
    "        self.q1_target = QNetwork(state_dim, action_dim, hidden_sizes)\n",
    "        self.q2_target = QNetwork(state_dim, action_dim, hidden_sizes)\n",
    "        self.policy = GaussianPolicy(state_dim, action_dim, hidden_sizes)\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "    @torch.no_grad()\n",
    "    def soft_update(self, tau=0.005):\n",
    "        for target_param, param in zip(self.q1_target.parameters(), self.q1.parameters()):\n",
    "            target_param.data.mul_(1 - tau).add_(tau * param.data)\n",
    "        for target_param, param in zip(self.q2_target.parameters(), self.q2.parameters()):\n",
    "            target_param.data.mul_(1 - tau).add_(tau * param.data)\n",
    "\n",
    "# 数据列定义\n",
    "STATE_COLS = [\n",
    "    \"vanco_level(ug/mL)\",\n",
    "    \"creatinine(mg/dL)\",\n",
    "    \"wbc(K/uL)\",\n",
    "    \"bun(mg/dL)\",\n",
    "    \"temperature\",\n",
    "    \"sbp\",\n",
    "    \"heart_rate\"\n",
    "]\n",
    "ACTION_COL = \"totalamount_mg\"\n",
    "REWARD_COL = \"step_reward\"\n",
    "TIME_COLS = [\"stay_id\", \"step_4hr\"]\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== 1. 加载模型和数据 ==========\n",
    "print(\"=\" * 80)\n",
    "print(\"CQL连续动作空间模型评估\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. 加载模型...\")\n",
    "checkpoint = torch.load('cql_final_model.pt', map_location=DEVICE, weights_only=False)\n",
    "state_scaler = checkpoint['state_scaler']\n",
    "action_scaler = checkpoint['action_scaler']\n",
    "config = checkpoint['config']\n",
    "r_min = checkpoint['r_min']\n",
    "r_range = checkpoint['r_range']\n",
    "state_dim = checkpoint['state_dim']\n",
    "action_dim = checkpoint['action_dim']\n",
    "\n",
    "# 初始化模型\n",
    "agent = CQLAgent(state_dim, action_dim).to(DEVICE)\n",
    "agent.load_state_dict(checkpoint['agent'])\n",
    "agent.eval()\n",
    "\n",
    "print(f\"✅ 模型已加载\")\n",
    "print(f\"   状态维度: {state_dim}\")\n",
    "print(f\"   动作维度: {action_dim}\")\n",
    "print(f\"   配置: alpha={config['alpha']}, gamma={config['gamma']}\")\n",
    "\n",
    "# ========== 2. 加载和预处理数据 ==========\n",
    "print(\"\\n2. 加载数据...\")\n",
    "df = pd.read_csv(\"ready_data.csv\")\n",
    "df = df.sort_values(['stay_id', 'step_4hr']).reset_index(drop=True)\n",
    "df[STATE_COLS] = df[STATE_COLS].fillna(df[STATE_COLS].median())\n",
    "\n",
    "# 标准化\n",
    "full_states = state_scaler.transform(df[STATE_COLS].values)\n",
    "full_rewards = df[REWARD_COL].values.astype(np.float32)\n",
    "\n",
    "# 归一化奖励（使用训练时的参数）\n",
    "full_rewards = (full_rewards - r_min) / r_range\n",
    "\n",
    "# 构建dones\n",
    "full_dones = np.zeros(len(df), dtype=np.float32)\n",
    "for stay_id in df['stay_id'].unique():\n",
    "    stay_mask = df['stay_id'] == stay_id\n",
    "    stay_indices = np.where(stay_mask)[0]\n",
    "    if len(stay_indices) > 0:\n",
    "        full_dones[stay_indices[-1]] = 1.0\n",
    "\n",
    "print(f\"✅ 数据已加载\")\n",
    "print(f\"   总记录数: {len(df)}\")\n",
    "print(f\"   总episode数: {df['stay_id'].nunique()}\")\n",
    "print(f\"   奖励范围（归一化）: [{full_rewards.min():.4f}, {full_rewards.max():.4f}]\")\n",
    "\n",
    "# ========== 3. FQE评估函数 ==========\n",
    "def evaluate_policy_value_fqe(agent, states, rewards, dones, gamma=0.99, n_samples=10):\n",
    "    \"\"\"\n",
    "    使用Fitted Q Evaluation (FQE) 评估策略价值（连续动作空间）\n",
    "    \n",
    "    对于连续动作空间，使用蒙特卡洛估计：\n",
    "    V^π(s) ≈ (1/N) * Σ_i Q(s, a_i),  a_i ~ π(·|s)\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    states_tensor = torch.FloatTensor(states).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 多次采样取平均（蒙特卡洛估计）\n",
    "        policy_values_list = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            policy_actions, _, _ = agent.policy.sample(states_tensor)\n",
    "            q1_vals = agent.q1(states_tensor, policy_actions)\n",
    "            q2_vals = agent.q2(states_tensor, policy_actions)\n",
    "            q_vals = (q1_vals + q2_vals) / 2\n",
    "            policy_values_list.append(q_vals.cpu().numpy())\n",
    "        \n",
    "        # 平均Q值（策略价值）\n",
    "        policy_values = np.mean(policy_values_list, axis=0).flatten()\n",
    "    \n",
    "    # 按episode分组，计算每个episode的初始状态价值\n",
    "    episode_initial_values = []\n",
    "    episode_actual_returns = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    current_return = 0.0\n",
    "    current_discount = 1.0\n",
    "    episode_start_idx = 0\n",
    "    \n",
    "    for i in range(len(dones)):\n",
    "        current_return += current_discount * rewards[i]\n",
    "        current_discount *= gamma\n",
    "        \n",
    "        if dones[i] == 1 or i == len(dones) - 1:\n",
    "            if episode_start_idx < len(policy_values):\n",
    "                episode_initial_values.append(policy_values[episode_start_idx])\n",
    "                episode_actual_returns.append(current_return)\n",
    "                episode_lengths.append(i - episode_start_idx + 1)\n",
    "            \n",
    "            current_return = 0.0\n",
    "            current_discount = 1.0\n",
    "            episode_start_idx = i + 1\n",
    "    \n",
    "    return {\n",
    "        'policy_value_mean': np.mean(policy_values),\n",
    "        'policy_value_std': np.std(policy_values),\n",
    "        'episode_initial_values': episode_initial_values,\n",
    "        'episode_actual_returns': episode_actual_returns,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'mean_episode_value': np.mean(episode_initial_values) if episode_initial_values else 0.0,\n",
    "        'mean_episode_return': np.mean(episode_actual_returns) if episode_actual_returns else 0.0,\n",
    "        'num_episodes': len(episode_initial_values),\n",
    "    }\n",
    "\n",
    "def evaluate_behavior_policy_value(rewards, dones, gamma=0.99):\n",
    "    \"\"\"评估行为策略（数据中的策略）的价值\"\"\"\n",
    "    episode_returns = []\n",
    "    current_return = 0.0\n",
    "    current_discount = 1.0\n",
    "    \n",
    "    for i in range(len(rewards)):\n",
    "        current_return += current_discount * rewards[i]\n",
    "        current_discount *= gamma\n",
    "        \n",
    "        if dones[i] == 1 or i == len(rewards) - 1:\n",
    "            episode_returns.append(current_return)\n",
    "            current_return = 0.0\n",
    "            current_discount = 1.0\n",
    "    \n",
    "    return {\n",
    "        'mean_return': np.mean(episode_returns) if episode_returns else 0.0,\n",
    "        'std_return': np.std(episode_returns) if episode_returns else 0.0,\n",
    "        'episode_returns': episode_returns,\n",
    "        'num_episodes': len(episode_returns),\n",
    "    }\n",
    "\n",
    "# ========== 4. 执行评估 ==========\n",
    "print(\"\\n3. 执行FQE评估...\")\n",
    "fqe_results = evaluate_policy_value_fqe(\n",
    "    agent, full_states, full_rewards, full_dones,\n",
    "    gamma=config['gamma'], n_samples=10\n",
    ")\n",
    "\n",
    "print(f\"\\n【CQL策略价值（FQE评估）】\")\n",
    "print(f\"  策略期望价值: {fqe_results['mean_episode_value']:.4f} ± {np.std(fqe_results['episode_initial_values']):.4f}\")\n",
    "print(f\"  平均状态价值: {fqe_results['policy_value_mean']:.4f} ± {fqe_results['policy_value_std']:.4f}\")\n",
    "print(f\"  评估episode数: {fqe_results['num_episodes']}\")\n",
    "\n",
    "print(\"\\n4. 评估行为策略...\")\n",
    "behavior_results = evaluate_behavior_policy_value(full_rewards, full_dones, gamma=config['gamma'])\n",
    "\n",
    "print(f\"\\n【行为策略价值（实际数据）】\")\n",
    "print(f\"  实际平均回报: {behavior_results['mean_return']:.4f} ± {behavior_results['std_return']:.4f}\")\n",
    "print(f\"  评估episode数: {behavior_results['num_episodes']}\")\n",
    "\n",
    "# ========== 5. 策略改进分析 ==========\n",
    "print(\"\\n5. 策略改进分析...\")\n",
    "improvement = fqe_results['mean_episode_value'] - behavior_results['mean_return']\n",
    "relative_improvement = (improvement / abs(behavior_results['mean_return'])) * 100 if behavior_results['mean_return'] != 0 else 0.0\n",
    "\n",
    "print(f\"\\n【策略改进分析】\")\n",
    "print(f\"  绝对改进: {improvement:.4f}\")\n",
    "print(f\"  相对改进: {relative_improvement:.2f}%\")\n",
    "if improvement > 0:\n",
    "    print(f\"  解释: CQL策略比行为策略好 {relative_improvement:.2f}%\")\n",
    "elif improvement < 0:\n",
    "    print(f\"  解释: CQL策略比行为策略保守 {abs(relative_improvement):.2f}%（医疗中保守是优点）\")\n",
    "else:\n",
    "    print(f\"  解释: CQL策略与行为策略相当\")\n",
    "\n",
    "# ========== 6. Episode级别详细分析 ==========\n",
    "print(\"\\n6. Episode级别详细分析...\")\n",
    "agent.eval()\n",
    "episode_detailed_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for stay_id in df['stay_id'].unique():\n",
    "        stay_data = df[df['stay_id'] == stay_id].sort_values('step_4hr').reset_index(drop=True)\n",
    "        \n",
    "        if len(stay_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        stay_indices = np.where(df['stay_id'] == stay_id)[0]\n",
    "        stay_states = torch.FloatTensor(full_states[stay_indices]).to(DEVICE)\n",
    "        stay_rewards = full_rewards[stay_indices]\n",
    "        \n",
    "        # 策略动作和Q值（多次采样取平均）\n",
    "        policy_values_list = []\n",
    "        policy_actions_list = []\n",
    "        \n",
    "        for _ in range(10):  # 采样10次\n",
    "            policy_actions, _, _ = agent.policy.sample(stay_states)\n",
    "            q1_vals = agent.q1(stay_states, policy_actions)\n",
    "            q2_vals = agent.q2(stay_states, policy_actions)\n",
    "            q_vals = (q1_vals + q2_vals) / 2\n",
    "            policy_values_list.append(q_vals.cpu().numpy())\n",
    "            policy_actions_list.append(policy_actions.cpu().numpy())\n",
    "        \n",
    "        # 平均\n",
    "        policy_values = np.mean(policy_values_list, axis=0).flatten()\n",
    "        policy_actions_norm = np.mean(policy_actions_list, axis=0).flatten()\n",
    "        \n",
    "        # 初始状态价值\n",
    "        initial_value = policy_values[0]\n",
    "        \n",
    "        # 实际回报\n",
    "        actual_return = stay_rewards.sum()\n",
    "        discounted_return = sum(r * (config['gamma'] ** i) for i, r in enumerate(stay_rewards))\n",
    "        \n",
    "        # 数据动作（原始值）\n",
    "        data_actions_raw = stay_data[ACTION_COL].values\n",
    "        \n",
    "        # 策略动作（反标准化到原始值）\n",
    "        policy_actions_raw = action_scaler.inverse_transform(policy_actions_norm.reshape(-1, 1)).flatten()\n",
    "        policy_actions_raw = np.maximum(0, policy_actions_raw)  # 确保非负\n",
    "        \n",
    "        # 动作差异（MAE）\n",
    "        action_mae = np.mean(np.abs(policy_actions_raw - data_actions_raw))\n",
    "        \n",
    "        episode_detailed_results.append({\n",
    "            'stay_id': stay_id,\n",
    "            'length': len(stay_data),\n",
    "            'initial_value': initial_value,\n",
    "            'actual_return': actual_return,\n",
    "            'discounted_return': discounted_return,\n",
    "            'action_mae': action_mae,\n",
    "            'mean_policy_action': np.mean(policy_actions_raw),\n",
    "            'mean_data_action': np.mean(data_actions_raw),\n",
    "        })\n",
    "\n",
    "# 汇总统计\n",
    "print(f\"\\n【Episode级别统计】\")\n",
    "print(f\"  评估episode数: {len(episode_detailed_results)}\")\n",
    "print(f\"  平均episode长度: {np.mean([r['length'] for r in episode_detailed_results]):.1f} 步\")\n",
    "print(f\"  平均初始价值: {np.mean([r['initial_value'] for r in episode_detailed_results]):.4f}\")\n",
    "print(f\"  平均实际回报: {np.mean([r['actual_return'] for r in episode_detailed_results]):.4f}\")\n",
    "print(f\"  平均折扣回报: {np.mean([r['discounted_return'] for r in episode_detailed_results]):.4f}\")\n",
    "print(f\"  平均动作MAE: {np.mean([r['action_mae'] for r in episode_detailed_results]):.2f} mg\")\n",
    "print(f\"  平均策略动作: {np.mean([r['mean_policy_action'] for r in episode_detailed_results]):.2f} mg\")\n",
    "print(f\"  平均数据动作: {np.mean([r['mean_data_action'] for r in episode_detailed_results]):.2f} mg\")\n",
    "\n",
    "\n",
    "# ========== 8. 最终总结 ==========\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"评估总结\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n【策略价值（FQE评估）】\")\n",
    "print(f\"  CQL策略期望价值: {fqe_results['mean_episode_value']:.4f}\")\n",
    "print(f\"  行为策略实际回报: {behavior_results['mean_return']:.4f}\")\n",
    "print(f\"\\n【策略改进】\")\n",
    "print(f\"  {relative_improvement:.2f}%\")\n",
    "if improvement > 0:\n",
    "    print(f\"  CQL策略比行为策略好 {relative_improvement:.2f}%\")\n",
    "elif improvement < 0:\n",
    "    print(f\"  CQL策略比行为策略保守 {abs(relative_improvement):.2f}%（医疗中保守是优点）\")\n",
    "print(f\"\\n【关键发现】\")\n",
    "print(f\"  • 策略价值是评估RL策略的核心指标\")\n",
    "print(f\"  • 平均动作MAE: {np.mean([r['action_mae'] for r in episode_detailed_results]):.2f} mg\")\n",
    "print(f\"  • 在医疗场景中，保守的策略（价值略低）可能是优点\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✅ 评估完成！\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
