{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f0bb7f-655c-4302-ad24-bd13dc715f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "================================================================================\n",
      "1. 加载数据...\n",
      "数据形状: (2113, 17)\n",
      "唯一stay_id数: 58\n",
      "\n",
      "动作统计:\n",
      "count    2113.000000\n",
      "mean       72.404638\n",
      "std       221.459863\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max      1500.000000\n",
      "Name: totalamount_mg, dtype: float64\n",
      "\n",
      "奖励统计:\n",
      "count    2113.000000\n",
      "mean       -0.734737\n",
      "std         1.173248\n",
      "min        -5.700000\n",
      "25%        -1.700000\n",
      "50%        -0.700000\n",
      "75%         0.200000\n",
      "max         2.500000\n",
      "Name: step_reward, dtype: float64\n",
      "\n",
      "2. 数据预处理...\n",
      "训练集: 1805 条记录, 46 个stay_id\n",
      "验证集: 308 条记录, 12 个stay_id\n",
      "状态维度: 7\n",
      "动作范围（原始）: [0.0, 1500.0] mg\n",
      "\n",
      "3. 构建数据集...\n",
      "\n",
      "4. 定义模型...\n",
      "初始化后Q1值范围: [-0.0028, 0.0006]\n",
      "初始化后Q2值范围: [-0.0004, 0.0023]\n",
      "状态维度: 7, 动作维度: 1\n",
      "模型参数数量: 341,766\n",
      "\n",
      "5. 定义CQL损失函数...\n",
      "\n",
      "6. 训练配置...\n",
      "\n",
      "7. 开始训练...\n",
      "================================================================================\n",
      "Epoch   1 | 训练损失: 39.0913\n",
      "Epoch   2 | 训练损失: 37.2220 | 验证损失: 41.1803 ✓ (最佳模型已保存)\n",
      "Epoch   3 | 训练损失: 37.7510\n",
      "Epoch   4 | 训练损失: 38.0299 | 验证损失: 40.5099 ✓ (最佳模型已保存)\n",
      "Epoch   5 | 训练损失: 38.2090\n",
      "Epoch   6 | 训练损失: 38.6193 | 验证损失: 44.1039 (耐心: 1/10)\n",
      "Epoch   7 | 训练损失: 38.8604\n",
      "Epoch   8 | 训练损失: 39.1653 | 验证损失: 45.9939 (耐心: 2/10)\n",
      "Epoch   9 | 训练损失: 39.2579\n",
      "Epoch  10 | 训练损失: 39.2241 | 验证损失: 46.1489 (耐心: 3/10)\n",
      "Epoch  11 | 训练损失: 39.5466\n",
      "Epoch  12 | 训练损失: 39.5203 | 验证损失: 50.1610 (耐心: 4/10)\n",
      "Epoch  13 | 训练损失: 39.9366\n",
      "Epoch  14 | 训练损失: 39.5734 | 验证损失: 52.1783 (耐心: 5/10)\n",
      "Epoch  15 | 训练损失: 40.1466\n",
      "Epoch  16 | 训练损失: 40.0732 | 验证损失: 53.4813 (耐心: 6/10)\n",
      "Epoch  17 | 训练损失: 40.1326\n",
      "Epoch  18 | 训练损失: 40.1390 | 验证损失: 55.9072 (耐心: 7/10)\n",
      "Epoch  19 | 训练损失: 40.1537\n",
      "Epoch  20 | 训练损失: 40.4886 | 验证损失: 57.5609 (耐心: 8/10)\n",
      "Epoch  21 | 训练损失: 40.4218\n",
      "Epoch  22 | 训练损失: 40.4650 | 验证损失: 54.9809 (耐心: 9/10)\n",
      "Epoch  23 | 训练损失: 40.5812\n",
      "Epoch  24 | 训练损失: 40.8058 | 验证损失: 56.0519 (耐心: 10/10)\n",
      "\n",
      "早停触发！最佳验证损失: 40.5099\n",
      "提示：验证损失只是训练指标，策略评估请运行 evaluate_cql_continuous.py\n",
      "\n",
      "训练完成！最佳验证损失: 40.5099\n",
      "\n",
      "8. 保存最终模型...\n",
      "✅ 模型已保存: cql_final_model.pt\n",
      "\n",
      "================================================================================\n",
      "重要提示：\n",
      "================================================================================\n",
      "1. 验证损失只是训练过程的监控指标（防止过拟合）\n",
      "2. 验证集的动作不一定是最优的，所以验证损失不能评估策略好坏\n",
      "3. 真正的策略评估应该使用FQE方法（策略价值）\n",
      "4. 运行评估脚本: python evaluate_cql_continuous.py\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CQL (Conservative Q-Learning) - 连续动作空间训练脚本\n",
    "ICU药物剂量优化 - 离线强化学习\n",
    "\n",
    "特点：\n",
    "- 连续动作空间（药物剂量 mg）\n",
    "- 以stay_id为轨迹，每4小时为一个状态\n",
    "- 使用高斯策略 + Twin Q网络\n",
    "- 包含策略评估（FQE）和推理接口\n",
    "\n",
    "运行方式：\n",
    "1. 直接运行: python cql_continuous_train.py\n",
    "2. 或在Jupyter Notebook中按cell执行\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict, List\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========== 配置 ==========\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 数据列定义\n",
    "STATE_COLS = [\n",
    "    \"vanco_level(ug/mL)\",\n",
    "    \"creatinine(mg/dL)\",\n",
    "    \"wbc(K/uL)\",\n",
    "    \"bun(mg/dL)\",\n",
    "    \"temperature\",\n",
    "    \"sbp\",\n",
    "    \"heart_rate\"\n",
    "]\n",
    "ACTION_COL = \"totalamount_mg\"\n",
    "REWARD_COL = \"step_reward\"\n",
    "TIME_COLS = [\"stay_id\", \"step_4hr\"]\n",
    "\n",
    "print(f\"使用设备: {DEVICE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ========== 1. 数据加载 ==========\n",
    "print(\"1. 加载数据...\")\n",
    "df = pd.read_csv(\"ready_data.csv\")\n",
    "print(f\"数据形状: {df.shape}\")\n",
    "print(f\"唯一stay_id数: {df['stay_id'].nunique()}\")\n",
    "print(f\"\\n动作统计:\")\n",
    "print(df[ACTION_COL].describe())\n",
    "print(f\"\\n奖励统计:\")\n",
    "print(df[REWARD_COL].describe())\n",
    "\n",
    "# ========== 2. 数据预处理 ==========\n",
    "print(\"\\n2. 数据预处理...\")\n",
    "df[STATE_COLS] = df[STATE_COLS].fillna(df[STATE_COLS].median())\n",
    "\n",
    "# 按stay_id划分训练集和验证集\n",
    "# 注意：验证集的作用是防止过拟合，不是评估策略好坏\n",
    "# - 验证集的动作不一定是最优的\n",
    "# - 验证损失只是训练过程的监控指标\n",
    "# - 真正的策略评估应该用FQE（策略价值），见 evaluate_cql_continuous.py\n",
    "stay_ids = df[\"stay_id\"].unique()\n",
    "train_stay_ids, val_stay_ids = train_test_split(stay_ids, test_size=0.2, random_state=42)\n",
    "train_df = df[df[\"stay_id\"].isin(train_stay_ids)].reset_index(drop=True)\n",
    "val_df = df[df[\"stay_id\"].isin(val_stay_ids)].reset_index(drop=True)\n",
    "\n",
    "print(f\"训练集: {len(train_df)} 条记录, {len(train_stay_ids)} 个stay_id\")\n",
    "print(f\"验证集: {len(val_df)} 条记录, {len(val_stay_ids)} 个stay_id\")\n",
    "\n",
    "# 标准化\n",
    "state_scaler = StandardScaler()\n",
    "train_states = state_scaler.fit_transform(train_df[STATE_COLS])\n",
    "val_states = state_scaler.transform(val_df[STATE_COLS])\n",
    "\n",
    "action_scaler = StandardScaler()\n",
    "train_actions = action_scaler.fit_transform(train_df[[ACTION_COL]]).flatten()\n",
    "val_actions = action_scaler.transform(val_df[[ACTION_COL]]).flatten()\n",
    "\n",
    "# 奖励归一化\n",
    "r_min = train_df[REWARD_COL].min()\n",
    "r_max = train_df[REWARD_COL].max()\n",
    "r_range = r_max - r_min if r_max != r_min else 1.0\n",
    "train_rewards = (train_df[REWARD_COL].values - r_min) / r_range\n",
    "val_rewards = (val_df[REWARD_COL].values - r_min) / r_range\n",
    "\n",
    "print(f\"状态维度: {train_states.shape[1]}\")\n",
    "print(f\"动作范围（原始）: [{train_df[ACTION_COL].min():.1f}, {train_df[ACTION_COL].max():.1f}] mg\")\n",
    "\n",
    "# 构建转移\n",
    "def build_transitions(df_src, scaled_states):\n",
    "    next_states = np.zeros_like(scaled_states)\n",
    "    dones = np.zeros(len(df_src), dtype=np.float32)\n",
    "    for stay_id in df_src['stay_id'].unique():\n",
    "        stay_mask = df_src['stay_id'] == stay_id\n",
    "        stay_data = df_src[stay_mask].sort_values('step_4hr').reset_index(drop=True)\n",
    "        stay_indices = np.where(stay_mask)[0]\n",
    "        for i, idx in enumerate(stay_indices):\n",
    "            if i < len(stay_indices) - 1:\n",
    "                next_states[idx] = scaled_states[stay_indices[i + 1]]\n",
    "                dones[idx] = 0.0\n",
    "            else:\n",
    "                next_states[idx] = scaled_states[idx]\n",
    "                dones[idx] = 1.0\n",
    "    return next_states.astype(np.float32), dones.astype(np.float32)\n",
    "\n",
    "train_next_states, train_dones = build_transitions(train_df, train_states)\n",
    "val_next_states, val_dones = build_transitions(val_df, val_states)\n",
    "\n",
    "# ========== 3. 数据集 ==========\n",
    "print(\"\\n3. 构建数据集...\")\n",
    "class RLDataset(Dataset):\n",
    "    def __init__(self, states, actions, rewards, next_states, dones):\n",
    "        self.states = torch.FloatTensor(states)\n",
    "        self.actions = torch.FloatTensor(actions)\n",
    "        self.rewards = torch.FloatTensor(rewards)\n",
    "        self.next_states = torch.FloatTensor(next_states)\n",
    "        self.dones = torch.FloatTensor(dones)\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.states[idx], self.actions[idx], self.rewards[idx], \n",
    "                self.next_states[idx], self.dones[idx])\n",
    "\n",
    "train_dataset = RLDataset(train_states, train_actions, train_rewards, train_next_states, train_dones)\n",
    "val_dataset = RLDataset(val_states, val_actions, val_rewards, val_next_states, val_dones)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, drop_last=False)\n",
    "\n",
    "# ========== 4. 模型定义 ==========\n",
    "print(\"\\n4. 定义模型...\")\n",
    "def create_mlp(input_dim, output_dim, hidden_sizes=(256, 256), activation=nn.ReLU):\n",
    "    layers = []\n",
    "    last_dim = input_dim\n",
    "    for h in hidden_sizes:\n",
    "        layers.extend([nn.Linear(last_dim, h), activation()])\n",
    "        last_dim = h\n",
    "    layers.append(nn.Linear(last_dim, output_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    \"\"\"高斯策略（连续动作空间）\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.net = create_mlp(state_dim, 2 * action_dim, hidden_sizes)\n",
    "        self.log_std_min = -5\n",
    "        self.log_std_max = 2\n",
    "    def forward(self, state):\n",
    "        mean_logstd = self.net(state)\n",
    "        mean, log_std = torch.chunk(mean_logstd, 2, dim=-1)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = self.log_std_min + 0.5 * (log_std + 1) * (self.log_std_max - self.log_std_min)\n",
    "        std = torch.exp(log_std)\n",
    "        return mean, std\n",
    "    def sample(self, state):\n",
    "        mean, std = self(state)\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()\n",
    "        action = torch.tanh(x_t)\n",
    "        log_prob = normal.log_prob(x_t) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "        return action, log_prob, torch.tanh(mean)\n",
    "    def get_deterministic_action(self, state):\n",
    "        mean, _ = self(state)\n",
    "        return torch.tanh(mean)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Q网络\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.net = create_mlp(state_dim + action_dim, 1, hidden_sizes)\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=-1)\n",
    "        return self.net(sa)\n",
    "\n",
    "class CQLAgent(nn.Module):\n",
    "    \"\"\"CQL Agent: Twin Q + 高斯策略\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.q1 = QNetwork(state_dim, action_dim, hidden_sizes)\n",
    "        self.q2 = QNetwork(state_dim, action_dim, hidden_sizes)\n",
    "        self.q1_target = QNetwork(state_dim, action_dim, hidden_sizes)\n",
    "        self.q2_target = QNetwork(state_dim, action_dim, hidden_sizes)\n",
    "        self.policy = GaussianPolicy(state_dim, action_dim, hidden_sizes)\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "    @torch.no_grad()\n",
    "    def soft_update(self, tau=0.005):\n",
    "        for target_param, param in zip(self.q1_target.parameters(), self.q1.parameters()):\n",
    "            target_param.data.mul_(1 - tau).add_(tau * param.data)\n",
    "        for target_param, param in zip(self.q2_target.parameters(), self.q2.parameters()):\n",
    "            target_param.data.mul_(1 - tau).add_(tau * param.data)\n",
    "\n",
    "state_dim = train_states.shape[1]\n",
    "action_dim = 1\n",
    "agent = CQLAgent(state_dim, action_dim).to(DEVICE)\n",
    "\n",
    "# ========== 修复：重新初始化Q网络为小值 ==========\n",
    "# 默认初始化可能导致Q值过大或过小，导致训练不稳定\n",
    "def init_q_network_small(m):\n",
    "    \"\"\"将Q网络初始化为接近0的小值，提高训练稳定性\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        if m.out_features == 1:  # Q网络输出层\n",
    "            # 输出层初始化为接近0的小值\n",
    "            nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "        else:\n",
    "            # 隐藏层使用较小的Xavier初始化\n",
    "            nn.init.xavier_uniform_(m.weight, gain=0.5)\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "agent.q1.apply(init_q_network_small)\n",
    "agent.q2.apply(init_q_network_small)\n",
    "agent.q1_target.load_state_dict(agent.q1.state_dict())\n",
    "agent.q2_target.load_state_dict(agent.q2.state_dict())\n",
    "\n",
    "# 验证初始化\n",
    "agent.eval()\n",
    "with torch.no_grad():\n",
    "    sample_state = torch.FloatTensor(train_states[:5]).to(DEVICE)\n",
    "    sample_action = torch.FloatTensor(train_actions[:5]).unsqueeze(1).to(DEVICE)\n",
    "    q1_init = agent.q1(sample_state, sample_action)\n",
    "    q2_init = agent.q2(sample_state, sample_action)\n",
    "    print(f\"初始化后Q1值范围: [{q1_init.min().item():.4f}, {q1_init.max().item():.4f}]\")\n",
    "    print(f\"初始化后Q2值范围: [{q2_init.min().item():.4f}, {q2_init.max().item():.4f}]\")\n",
    "agent.train()\n",
    "\n",
    "print(f\"状态维度: {state_dim}, 动作维度: {action_dim}\")\n",
    "print(f\"模型参数数量: {sum(p.numel() for p in agent.parameters()):,}\")\n",
    "\n",
    "# ========== 5. CQL损失函数 ==========\n",
    "print(\"\\n5. 定义CQL损失函数...\")\n",
    "def compute_cql_loss(agent, batch, config):\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "    states = states.to(DEVICE)\n",
    "    actions = actions.unsqueeze(1).to(DEVICE)\n",
    "    rewards = rewards.unsqueeze(1).to(DEVICE)\n",
    "    next_states = next_states.to(DEVICE)\n",
    "    dones = dones.unsqueeze(1).to(DEVICE)\n",
    "    \n",
    "    # Bellman误差\n",
    "    q1_data = agent.q1(states, actions)\n",
    "    q2_data = agent.q2(states, actions)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_actions, next_logp, _ = agent.policy.sample(next_states)\n",
    "        next_q1_target = agent.q1_target(next_states, next_actions)\n",
    "        next_q2_target = agent.q2_target(next_states, next_actions)\n",
    "        next_q_target = torch.min(next_q1_target, next_q2_target) - next_logp\n",
    "        backup = rewards + config['gamma'] * (1 - dones) * next_q_target\n",
    "    \n",
    "    q1_loss = F.huber_loss(q1_data, backup, delta=1.0)\n",
    "    q2_loss = F.huber_loss(q2_data, backup, delta=1.0)\n",
    "    \n",
    "    # CQL正则项\n",
    "    batch_size = states.shape[0]\n",
    "    cql_samples = config.get('cql_samples', 10)\n",
    "    random_actions = torch.empty(batch_size, cql_samples, action_dim, device=DEVICE).uniform_(-1, 1)\n",
    "    policy_actions, _, _ = agent.policy.sample(states)\n",
    "    states_rep = states.unsqueeze(1).expand(-1, cql_samples, -1).reshape(-1, states.shape[-1])\n",
    "    random_actions_flat = random_actions.reshape(-1, action_dim)\n",
    "    q1_rand = agent.q1(states_rep, random_actions_flat).reshape(batch_size, cql_samples, 1)\n",
    "    q2_rand = agent.q2(states_rep, random_actions_flat).reshape(batch_size, cql_samples, 1)\n",
    "    q1_policy = agent.q1(states, policy_actions)\n",
    "    q2_policy = agent.q2(states, policy_actions)\n",
    "    q1_cat = torch.cat([q1_rand, q1_policy.unsqueeze(1)], dim=1)\n",
    "    q2_cat = torch.cat([q2_rand, q2_policy.unsqueeze(1)], dim=1)\n",
    "    cql1 = torch.logsumexp(q1_cat, dim=1).mean() - q1_data.mean()\n",
    "    cql2 = torch.logsumexp(q2_cat, dim=1).mean() - q2_data.mean()\n",
    "    \n",
    "    # 策略损失\n",
    "    policy_dist = torch.distributions.Normal(*agent.policy(states))\n",
    "    policy_log_prob = policy_dist.log_prob(actions.squeeze(1)).sum(dim=-1, keepdim=True)\n",
    "    policy_loss = -policy_log_prob.mean()\n",
    "    \n",
    "    total_loss = q1_loss + q2_loss + config['alpha'] * (cql1 + cql2) + 0.1 * policy_loss\n",
    "    \n",
    "    info = {\n",
    "        'total_loss': total_loss.item(),\n",
    "        'q_loss': (q1_loss + q2_loss).item(),\n",
    "        'cql1': cql1.item(),\n",
    "        'cql2': cql2.item(),\n",
    "        'policy_loss': policy_loss.item(),\n",
    "        'q1_mean': q1_data.mean().item(),\n",
    "    }\n",
    "    return total_loss, info\n",
    "\n",
    "# ========== 6. 训练配置 ==========\n",
    "print(\"\\n6. 训练配置...\")\n",
    "config = {\n",
    "    'batch_size': 256,\n",
    "    'lr': 1e-4,\n",
    "    'gamma': 0.99,\n",
    "    'alpha': 0.01,  # 降低到0.01，避免Q值被CQL正则项拉低\n",
    "    'tau': 0.005,\n",
    "    'cql_samples': 10,\n",
    "    'epochs': 30,\n",
    "    'steps_per_epoch': 1000,\n",
    "    'val_interval': 2,\n",
    "}\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    list(agent.q1.parameters()) + \n",
    "    list(agent.q2.parameters()) + \n",
    "    list(agent.policy.parameters()),\n",
    "    lr=config['lr']\n",
    ")\n",
    "\n",
    "# ========== 7. 训练循环 ==========\n",
    "print(\"\\n7. 开始训练...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def evaluate(agent, val_loader, config):\n",
    "    \"\"\"\n",
    "    评估验证集损失（用于防止过拟合，不是策略评估）\n",
    "    \n",
    "    注意：\n",
    "    - 验证集的动作不一定是最优的\n",
    "    - 验证损失只是训练过程的监控指标\n",
    "    - 真正的策略评估应该用FQE（见 evaluate_cql_continuous.py）\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            loss, _ = compute_cql_loss(agent, batch, config)\n",
    "            losses.append(loss.item())\n",
    "    agent.train()\n",
    "    return np.mean(losses)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    agent.train()\n",
    "    epoch_losses = []\n",
    "    for step in range(config['steps_per_epoch']):\n",
    "        batch = next(iter(train_loader))\n",
    "        loss, info = compute_cql_loss(agent, batch, config)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(agent.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        agent.soft_update(config['tau'])\n",
    "        epoch_losses.append(info['total_loss'])\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if epoch % config['val_interval'] == 0:\n",
    "        # 验证损失用于防止过拟合，选择最佳模型\n",
    "        # 注意：这不是策略评估！真正的策略评估用FQE（见 evaluate_cql_continuous.py）\n",
    "        val_loss = evaluate(agent, val_loader, config)\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'agent': agent.state_dict(),\n",
    "                'state_scaler': state_scaler,\n",
    "                'action_scaler': action_scaler,\n",
    "                'config': config,\n",
    "                'r_min': r_min,\n",
    "                'r_range': r_range,\n",
    "            }, 'cql_best_model.pt')\n",
    "            print(f\"Epoch {epoch:3d} | 训练损失: {avg_loss:.4f} | 验证损失: {val_loss:.4f} ✓ (最佳模型已保存)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Epoch {epoch:3d} | 训练损失: {avg_loss:.4f} | 验证损失: {val_loss:.4f} (耐心: {patience_counter}/{patience})\")\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\n早停触发！最佳验证损失: {best_val_loss:.4f}\")\n",
    "                print(\"提示：验证损失只是训练指标，策略评估请运行 evaluate_cql_continuous.py\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:3d} | 训练损失: {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"\\n训练完成！最佳验证损失: {best_val_loss:.4f}\")\n",
    "\n",
    "# ========== 8. 保存最终模型 ==========\n",
    "print(\"\\n8. 保存最终模型...\")\n",
    "torch.save({\n",
    "    'agent': agent.state_dict(),\n",
    "    'state_scaler': state_scaler,\n",
    "    'action_scaler': action_scaler,\n",
    "    'config': config,\n",
    "    'r_min': r_min,\n",
    "    'r_range': r_range,\n",
    "    'state_dim': state_dim,\n",
    "    'action_dim': action_dim,\n",
    "    'STATE_COLS': STATE_COLS,\n",
    "    'ACTION_COL': ACTION_COL,\n",
    "    'REWARD_COL': REWARD_COL,\n",
    "}, 'cql_final_model.pt')\n",
    "\n",
    "print(\"✅ 模型已保存: cql_final_model.pt\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"重要提示：\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. 验证损失只是训练过程的监控指标（防止过拟合）\")\n",
    "print(\"2. 验证集的动作不一定是最优的，所以验证损失不能评估策略好坏\")\n",
    "print(\"3. 真正的策略评估应该使用FQE方法（策略价值）\")\n",
    "print(\"4. 运行评估脚本: python evaluate_cql_continuous.py\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
