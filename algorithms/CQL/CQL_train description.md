# CQL (Conservative Q-Learning) - è¿ç»­åŠ¨ä½œç©ºé—´æŠ€æœ¯æ–‡æ¡£

## ğŸ“š ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [ç®—æ³•åŸç†](#ç®—æ³•åŸç†)
3. [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
4. [æ•°æ®æ ¼å¼](#æ•°æ®æ ¼å¼)
5. [æ¨¡å‹å®ç°](#æ¨¡å‹å®ç°)
6. [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
7. [ç­–ç•¥è¯„ä¼°](#ç­–ç•¥è¯„ä¼°)
8. [æ¨ç†æ¥å£](#æ¨ç†æ¥å£)
9. [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
10. [æŠ€æœ¯ç»†èŠ‚](#æŠ€æœ¯ç»†èŠ‚)

---

## é¡¹ç›®æ¦‚è¿°

### åº”ç”¨åœºæ™¯
ICUï¼ˆé‡ç—‡ç›‘æŠ¤å®¤ï¼‰è¯ç‰©å‰‚é‡ä¼˜åŒ–ç³»ç»Ÿï¼Œä½¿ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä»å†å²æ•°æ®ä¸­å­¦ä¹ æœ€ä¼˜ç»™è¯ç­–ç•¥ã€‚

### æ ¸å¿ƒç‰¹ç‚¹
- **è¿ç»­åŠ¨ä½œç©ºé—´**ï¼šè¯ç‰©å‰‚é‡ï¼ˆmgï¼‰æ˜¯è¿ç»­å€¼ï¼Œä¸æ˜¯ç¦»æ•£åˆ†ç±»
- **è½¨è¿¹ç»“æ„**ï¼šä»¥`stay_id`ï¼ˆæ‚£è€…ICUä½é™¢è®°å½•ï¼‰ä¸ºè½¨è¿¹ï¼Œæ¯4å°æ—¶ä¸ºä¸€ä¸ªçŠ¶æ€
- **ç¦»çº¿å­¦ä¹ **ï¼šåªä½¿ç”¨å†å²æ•°æ®ï¼Œä¸ä¸ç¯å¢ƒäº¤äº’
- **ä¿å®ˆç­–ç•¥**ï¼šCQLç¡®ä¿å­¦åˆ°çš„ç­–ç•¥æ¯”æ•°æ®ä¸­çš„ç­–ç•¥æ›´ä¿å®ˆã€æ›´å®‰å…¨

### æŠ€æœ¯æ ˆ
- **ç®—æ³•**ï¼šConservative Q-Learning (CQL)
- **æ¡†æ¶**ï¼šPyTorch
- **ç­–ç•¥ç±»å‹**ï¼šé«˜æ–¯ç­–ç•¥ï¼ˆGaussian Policyï¼‰
- **Qç½‘ç»œ**ï¼šTwin Qç½‘ç»œï¼ˆé˜²æ­¢è¿‡ä¼°è®¡ï¼‰

---

## ç®—æ³•åŸç†

### å¼ºåŒ–å­¦ä¹ åŸºç¡€

åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬åªæœ‰å†å²æ•°æ®ï¼š
```
å†å²æ•°æ®: (s, a, r, s', done) - åŒ»ç”Ÿè¿‡å»çš„ç»™è¯è®°å½•
ç›®æ ‡: ä»è¿™äº›æ•°æ®ä¸­å­¦ä¹ ä¸€ä¸ªç­–ç•¥ Ï€ï¼Œæ¯”åŒ»ç”Ÿæ›´å®‰å…¨ã€æ›´æœ‰æ•ˆ
æŒ‘æˆ˜: ä¸èƒ½å°è¯•æ–°ç­–ç•¥ï¼Œå¿…é¡»ä¿å®ˆä¼°è®¡
```

### CQLæ ¸å¿ƒæ€æƒ³

**æ ‡å‡†Q-learningé—®é¢˜**ï¼š
- Q(s,a) å¯èƒ½å¯¹æœªè§è¿‡çš„åŠ¨ä½œè¿‡äºä¹è§‚
- å¯¼è‡´ç­–ç•¥é€‰æ‹©å±é™©çš„åŠ¨ä½œ

**CQLçš„è§£å†³æ–¹æ¡ˆ**ï¼š
- æƒ©ç½šæœªè§è¿‡çš„åŠ¨ä½œï¼Œå¥–åŠ±æ•°æ®ä¸­çš„åŠ¨ä½œ
- å­¦åˆ°çš„ç­–ç•¥æ›´ä¿å®ˆã€æ›´å®‰å…¨

### è¿ç»­åŠ¨ä½œç©ºé—´çš„CQL

#### 1. é«˜æ–¯ç­–ç•¥ï¼ˆGaussian Policyï¼‰

å¯¹äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œä½¿ç”¨é«˜æ–¯åˆ†å¸ƒï¼š

```
Ï€(a|s) = N(Î¼(s), Ïƒ(s))

å…¶ä¸­:
  Î¼(s): ç­–ç•¥ç½‘ç»œè¾“å‡ºçš„å‡å€¼
  Ïƒ(s): ç­–ç•¥ç½‘ç»œè¾“å‡ºçš„æ ‡å‡†å·®ï¼ˆç»è¿‡tanhé™åˆ¶ï¼‰
  
åŠ¨ä½œé‡‡æ ·:
  a ~ Ï€(a|s) = tanh(N(Î¼, Ïƒ))  # é™åˆ¶åˆ°[-1, 1]
```

**é‡å‚æ•°åŒ–æŠ€å·§**ï¼š
```
a = tanh(Î¼ + Ïƒ * Îµ),  Îµ ~ N(0, 1)
```

#### 2. Twin Qç½‘ç»œ

```
Q1(s, a) å’Œ Q2(s, a) ä¸¤ä¸ªç‹¬ç«‹ç½‘ç»œ
ç›®æ ‡: min(Q1, Q2) - log Ï€(a|s)  # SACé£æ ¼
```

#### 3. CQLæŸå¤±å‡½æ•°ï¼ˆè¿ç»­åŠ¨ä½œç©ºé—´ï¼‰

```
CQLæŸå¤± = Bellmanè¯¯å·® + Î± * CQLæ­£åˆ™é¡¹ + ç­–ç•¥æŸå¤±

å…¶ä¸­:
  Bellmanè¯¯å·®: Huber(Q(s,a), r + Î³ * E[Q(s',a')])
  
  CQLæ­£åˆ™é¡¹ï¼ˆè¿ç»­ç‰ˆæœ¬ï¼‰:
    logsumexp(Q(s, a_samples)) - Q(s, a_data)
    
    å…¶ä¸­ a_samples åŒ…æ‹¬:
      - éšæœºåŠ¨ä½œï¼ˆåœ¨[-1,1]èŒƒå›´å†…å‡åŒ€é‡‡æ ·ï¼‰
      - ç­–ç•¥åŠ¨ä½œï¼ˆä»Ï€(Â·|s)é‡‡æ ·ï¼‰
    
  ç­–ç•¥æŸå¤±:
    -log Ï€(a_data|s)  # è¡Œä¸ºå…‹éš†é¡¹
```

**å…³é”®ç‚¹**ï¼š
- è¿ç»­åŠ¨ä½œç©ºé—´æ— æ³•æšä¸¾æ‰€æœ‰åŠ¨ä½œ
- ä½¿ç”¨é‡‡æ ·æ–¹æ³•ï¼šéšæœºåŠ¨ä½œ + ç­–ç•¥åŠ¨ä½œ
- logsumexpå¯¹æ‰€æœ‰é‡‡æ ·åŠ¨ä½œçš„Qå€¼å–logsumexp

---

## ç³»ç»Ÿæ¶æ„

### ç½‘ç»œç»“æ„

```
è¾“å…¥å±‚ (7ç»´æ‚£è€…çŠ¶æ€)
    â†“
    â”œâ”€â†’ Q1ç½‘ç»œ â”€â†’ Q1(s, a) âˆˆ R
    â”‚   è¾“å…¥: [çŠ¶æ€, åŠ¨ä½œ]
    â”‚   è¾“å‡º: Qå€¼
    â”‚
    â”œâ”€â†’ Q2ç½‘ç»œ â”€â†’ Q2(s, a) âˆˆ R
    â”‚   è¾“å…¥: [çŠ¶æ€, åŠ¨ä½œ]
    â”‚   è¾“å‡º: Qå€¼
    â”‚
    â”œâ”€â†’ Q1_target â”€â†’ ç›®æ ‡Q1ç½‘ç»œï¼ˆè½¯æ›´æ–°ï¼‰
    â”‚
    â”œâ”€â†’ Q2_target â”€â†’ ç›®æ ‡Q2ç½‘ç»œï¼ˆè½¯æ›´æ–°ï¼‰
    â”‚
    â””â”€â†’ Policyç½‘ç»œ â”€â†’ Ï€(a|s) = N(Î¼(s), Ïƒ(s))
        è¾“å…¥: çŠ¶æ€
        è¾“å‡º: [å‡å€¼, æ ‡å‡†å·®]
```

### æ•°æ®æµ

```
åŸå§‹CSV (ready_data.csv)
    â†“
æ•°æ®é¢„å¤„ç†
    â”œâ”€ å¡«å……ç¼ºå¤±å€¼ï¼ˆä¸­ä½æ•°å¡«å……ï¼‰
    â”œâ”€ æŒ‰stay_idåˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼ˆ80/20ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    â”œâ”€ æ ‡å‡†åŒ–çŠ¶æ€ç‰¹å¾å’ŒåŠ¨ä½œï¼ˆStandardScalerï¼‰
    â””â”€ å¥–åŠ±å½’ä¸€åŒ–ï¼ˆèŒƒå›´[0, 1]ï¼‰
    â†“
æ„å»ºè½¬ç§» (s, a, r, s', done)
    â”œâ”€ çŠ¶æ€: 7ç»´ç‰¹å¾å‘é‡
    â”œâ”€ åŠ¨ä½œ: è¿ç»­å€¼ï¼ˆmgï¼‰ï¼Œæ ‡å‡†åŒ–åˆ°[-1,1]
    â”œâ”€ å¥–åŠ±: step_rewardï¼ˆå·²å½’ä¸€åŒ–ï¼‰
    â”œâ”€ ä¸‹ä¸€çŠ¶æ€: é€šè¿‡stay_id+step_4hrå¯¹é½
    â””â”€ doneæ ‡è®°: stay_idå˜åŒ–æ—¶=1
    â†“
DataLoader (batch_size=256)
    â†“
Qç½‘ç»œé‡æ–°åˆå§‹åŒ–ï¼ˆæ¥è¿‘0çš„å°å€¼ï¼‰
    â†“
è®­ç»ƒ (50 epochs, æ¯epoch 1000æ­¥)
    â”œâ”€ ç›‘æ§è®­ç»ƒæŸå¤±å’ŒQå€¼
    â”œâ”€ æ¯2ä¸ªepochéªŒè¯ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    â””â”€ æ—©åœæ£€æŸ¥ (patience=10)
    â†“
ä¿å­˜æœ€ä½³æ¨¡å‹
    â†“
ç­–ç•¥è¯„ä¼°ï¼ˆFQEæ–¹æ³•ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰
```

---

## æ•°æ®æ ¼å¼

### è¾“å…¥æ•°æ®ï¼ˆready_data.csvï¼‰

| åˆ—å | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| stay_id | int | æ‚£è€…ICUä½é™¢è®°å½•IDï¼ˆè½¨è¿¹æ ‡è¯†ï¼‰ |
| step_4hr | int | 4å°æ—¶æ—¶é—´æ­¥ |
| totalamount_mg | float | è¯ç‰©å‰‚é‡ï¼ˆmgï¼‰- **åŠ¨ä½œ** |
| vanco_level(ug/mL) | float | ä¸‡å¤éœ‰ç´ æµ“åº¦ - **çŠ¶æ€** |
| creatinine(mg/dL) | float | è‚Œé… - **çŠ¶æ€** |
| wbc(K/uL) | float | ç™½ç»†èƒè®¡æ•° - **çŠ¶æ€** |
| bun(mg/dL) | float | è¡€å°¿ç´ æ°® - **çŠ¶æ€** |
| temperature | float | ä½“æ¸© - **çŠ¶æ€** |
| sbp | float | æ”¶ç¼©å‹ - **çŠ¶æ€** |
| heart_rate | float | å¿ƒç‡ - **çŠ¶æ€** |
| step_reward | float | å³æ—¶å¥–åŠ± |

### çŠ¶æ€ç‰¹å¾ï¼ˆSTATE_COLSï¼‰

```python
STATE_COLS = [
    "vanco_level(ug/mL)",    # è¯ç‰©æµ“åº¦
    "creatinine(mg/dL)",      # è‚¾åŠŸèƒ½æŒ‡æ ‡
    "wbc(K/uL)",             # æ„ŸæŸ“æŒ‡æ ‡
    "bun(mg/dL)",            # è‚¾åŠŸèƒ½æŒ‡æ ‡
    "temperature",            # ä½“æ¸©
    "sbp",                   # æ”¶ç¼©å‹
    "heart_rate",            # å¿ƒç‡
]
```

### åŠ¨ä½œç©ºé—´

- **ç±»å‹**ï¼šè¿ç»­å€¼ï¼ˆæµ®ç‚¹æ•°ï¼‰
- **èŒƒå›´**ï¼š0 ~ æœ€å¤§å‰‚é‡ï¼ˆmgï¼‰
- **æ ‡å‡†åŒ–**ï¼šä½¿ç”¨StandardScaleræ ‡å‡†åŒ–åˆ°[-1, 1]èŒƒå›´
- **æ¨ç†æ—¶**ï¼šåæ ‡å‡†åŒ–å›åŸå§‹èŒƒå›´

### å¥–åŠ±ä¿¡å·

- **æ¥æº**ï¼š`step_reward`åˆ—
- **å½’ä¸€åŒ–**ï¼š`(reward - min) / (max - min)`ï¼ŒèŒƒå›´[0, 1]
- **å«ä¹‰**ï¼šæ­£å¥–åŠ±è¡¨ç¤ºå¥½çš„çŠ¶æ€ï¼Œè´Ÿå¥–åŠ±è¡¨ç¤ºéœ€è¦è°ƒæ•´

---

## æ¨¡å‹å®ç°

### 1. é«˜æ–¯ç­–ç•¥ç½‘ç»œ

```python
class GaussianPolicy(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_sizes=(256, 256)):
        super().__init__()
        self.net = create_mlp(state_dim, 2 * action_dim, hidden_sizes)
        self.log_std_min = -5
        self.log_std_max = 2
    
    def forward(self, state):
        mean_logstd = self.net(state)
        mean, log_std = torch.chunk(mean_logstd, 2, dim=-1)
        log_std = torch.tanh(log_std)
        log_std = self.log_std_min + 0.5 * (log_std + 1) * (self.log_std_max - self.log_std_min)
        std = torch.exp(log_std)
        return mean, std
    
    def sample(self, state):
        mean, std = self(state)
        normal = torch.distributions.Normal(mean, std)
        x_t = normal.rsample()  # é‡å‚æ•°åŒ–
        action = torch.tanh(x_t)  # é™åˆ¶åˆ°[-1, 1]
        log_prob = normal.log_prob(x_t) - torch.log(1 - action.pow(2) + 1e-6)
        log_prob = log_prob.sum(dim=-1, keepdim=True)
        return action, log_prob, torch.tanh(mean)
```

**å…³é”®ç‚¹**ï¼š
- è¾“å‡ºå‡å€¼å’Œæ ‡å‡†å·®
- ä½¿ç”¨tanhé™åˆ¶æ ‡å‡†å·®èŒƒå›´
- é‡å‚æ•°åŒ–æŠ€å·§ä½¿æ¢¯åº¦å¯ä¼ æ’­
- tanhå˜æ¢åéœ€è¦ä¿®æ­£logæ¦‚ç‡

### 2. Qç½‘ç»œ

```python
class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_sizes=(256, 256)):
        super().__init__()
        self.net = create_mlp(state_dim + action_dim, 1, hidden_sizes)
    
    def forward(self, state, action):
        sa = torch.cat([state, action], dim=-1)
        return self.net(sa)
```

**å…³é”®ç‚¹**ï¼š
- è¾“å…¥ï¼šçŠ¶æ€+åŠ¨ä½œçš„æ‹¼æ¥
- è¾“å‡ºï¼šå•ä¸ªQå€¼
- Twin Qï¼šä¸¤ä¸ªç‹¬ç«‹çš„Qç½‘ç»œ

### 3. CQL Agent

```python
class CQLAgent(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_sizes=(256, 256)):
        super().__init__()
        self.q1 = QNetwork(state_dim, action_dim, hidden_sizes)
        self.q2 = QNetwork(state_dim, action_dim, hidden_sizes)
        self.q1_target = QNetwork(state_dim, action_dim, hidden_sizes)
        self.q2_target = QNetwork(state_dim, action_dim, hidden_sizes)
        self.policy = GaussianPolicy(state_dim, action_dim, hidden_sizes)
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
        self.q1_target.load_state_dict(self.q1.state_dict())
        self.q2_target.load_state_dict(self.q2.state_dict())
```

### 4. Qç½‘ç»œåˆå§‹åŒ–ä¿®å¤ï¼ˆé‡è¦ï¼‰

**é—®é¢˜**ï¼šé»˜è®¤åˆå§‹åŒ–å¯èƒ½å¯¼è‡´Qå€¼è¿‡å¤§æˆ–è¿‡å°ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨æ¨¡å‹åˆå§‹åŒ–åé‡æ–°åˆå§‹åŒ–Qç½‘ç»œä¸ºå°å€¼

```python
def init_q_network_small(m):
    """å°†Qç½‘ç»œåˆå§‹åŒ–ä¸ºæ¥è¿‘0çš„å°å€¼ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§"""
    if isinstance(m, nn.Linear):
        if m.out_features == 1:  # Qç½‘ç»œè¾“å‡ºå±‚
            # è¾“å‡ºå±‚åˆå§‹åŒ–ä¸ºæ¥è¿‘0çš„å°å€¼
            nn.init.uniform_(m.weight, -0.01, 0.01)
            nn.init.constant_(m.bias, 0.0)
        else:
            # éšè—å±‚ä½¿ç”¨è¾ƒå°çš„Xavieråˆå§‹åŒ–
            nn.init.xavier_uniform_(m.weight, gain=0.5)
            nn.init.constant_(m.bias, 0.0)

agent.q1.apply(init_q_network_small)
agent.q2.apply(init_q_network_small)
agent.q1_target.load_state_dict(agent.q1.state_dict())
agent.q2_target.load_state_dict(agent.q2.state_dict())

# éªŒè¯åˆå§‹åŒ–
agent.eval()
with torch.no_grad():
    sample_state = torch.FloatTensor(train_states[:5]).to(DEVICE)
    sample_action = torch.FloatTensor(train_actions[:5]).unsqueeze(1).to(DEVICE)
    q1_init = agent.q1(sample_state, sample_action)
    q2_init = agent.q2(sample_state, sample_action)
    print(f"åˆå§‹åŒ–åQ1å€¼èŒƒå›´: [{q1_init.min().item():.4f}, {q1_init.max().item():.4f}]")
    print(f"åˆå§‹åŒ–åQ2å€¼èŒƒå›´: [{q2_init.min().item():.4f}, {q2_init.max().item():.4f}]")
agent.train()
```

**å…³é”®ç‚¹**ï¼š
- Qç½‘ç»œè¾“å‡ºå±‚åˆå§‹åŒ–ä¸ºæ¥è¿‘0çš„å°å€¼ï¼ˆ-0.01åˆ°0.01ï¼‰
- éšè—å±‚ä½¿ç”¨è¾ƒå°çš„Xavieråˆå§‹åŒ–ï¼ˆgain=0.5ï¼‰
- åˆå§‹åŒ–åéªŒè¯Qå€¼èŒƒå›´ï¼Œåº”è¯¥åœ¨[-0.01, 0.01]é™„è¿‘

---

## è®­ç»ƒæµç¨‹

### éªŒè¯é›†çš„ä½œç”¨ï¼ˆé‡è¦ï¼‰

**å…³é”®ç†è§£**ï¼šåœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒéªŒè¯é›†çš„ä½œç”¨ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¸åŒã€‚

| æ–¹é¢ | ä¼ ç»Ÿæœºå™¨å­¦ä¹  | ç¦»çº¿å¼ºåŒ–å­¦ä¹  |
|------|------------|------------|
| **éªŒè¯é›†ä½œç”¨** | è¯„ä¼°æ¨¡å‹åœ¨"æ­£ç¡®æ ‡ç­¾"ä¸Šçš„è¡¨ç° | é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé€‰æ‹©æœ€ä½³æ¨¡å‹ |
| **éªŒè¯æŒ‡æ ‡** | å‡†ç¡®ç‡ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰ | éªŒè¯æŸå¤±ï¼ˆè¶Šä½è¶Šå¥½ï¼Œä½†åªæ˜¯è®­ç»ƒæŒ‡æ ‡ï¼‰ |
| **åŠ¨ä½œæ˜¯å¦æœ€ä¼˜** | âœ… æ ‡ç­¾æ˜¯"æ­£ç¡®"çš„ | âŒ åŠ¨ä½œä¸ä¸€å®šæ˜¯æœ€ä¼˜çš„ |
| **ç­–ç•¥è¯„ä¼°** | ä½¿ç”¨éªŒè¯é›† | ä½¿ç”¨FQEæ–¹æ³•ï¼ˆå…¨éƒ¨æ•°æ®ï¼‰ |

**éªŒè¯é›†çš„ä½œç”¨**ï¼š
1. âœ… é˜²æ­¢è¿‡æ‹Ÿåˆï¼šæ£€æµ‹æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆåˆ°è®­ç»ƒæ•°æ®
2. âœ… é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼šä¿å­˜éªŒè¯æŸå¤±æœ€ä½çš„æ¨¡å‹
3. âœ… æ—©åœæœºåˆ¶ï¼šéªŒè¯æŸå¤±ä¸å†ä¸‹é™æ—¶åœæ­¢è®­ç»ƒ

**éªŒè¯é›†ä¸èƒ½åšä»€ä¹ˆ**ï¼š
- âŒ ä¸èƒ½è¯„ä¼°ç­–ç•¥å¥½åï¼ˆå› ä¸ºåŠ¨ä½œä¸ä¸€å®šæ˜¯æœ€ä¼˜çš„ï¼‰
- âŒ éªŒè¯æŸå¤±ä¸æ˜¯ç­–ç•¥è¯„ä¼°æŒ‡æ ‡

**çœŸæ­£çš„ç­–ç•¥è¯„ä¼°**ï¼š
- ä½¿ç”¨FQEæ–¹æ³•è¯„ä¼°ç­–ç•¥ä»·å€¼
- ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼ˆä¸åˆ’åˆ†è®­ç»ƒ/éªŒè¯ï¼‰
- ä¸è¡Œä¸ºç­–ç•¥å¯¹æ¯”

### è®­ç»ƒé…ç½®

```python
config = {
    'batch_size': 256,
    'lr': 1e-4,
    'gamma': 0.99,          # æŠ˜æ‰£å› å­
    'alpha': 0.01,          # CQLæ­£åˆ™ç³»æ•°ï¼ˆé™ä½åˆ°0.01ï¼Œé¿å…Qå€¼è¢«æ‹‰ä½ï¼‰
    'tau': 0.005,           # ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°ç³»æ•°
    'cql_samples': 10,      # CQLé‡‡æ ·åŠ¨ä½œæ•°
    'epochs': 50,
    'steps_per_epoch': 1000,
    'val_interval': 2,      # æ¯2ä¸ªepochéªŒè¯ä¸€æ¬¡
}
```

**é‡è¦è¯´æ˜**ï¼š
- **éªŒè¯é›†çš„ä½œç”¨**ï¼šéªŒè¯é›†ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä¸æ˜¯è¯„ä¼°ç­–ç•¥å¥½å
  - éªŒè¯é›†çš„åŠ¨ä½œä¸ä¸€å®šæ˜¯æœ€ä¼˜çš„
  - éªŒè¯æŸå¤±åªæ˜¯è®­ç»ƒè¿‡ç¨‹çš„ç›‘æ§æŒ‡æ ‡
  - çœŸæ­£çš„ç­–ç•¥è¯„ä¼°åº”è¯¥ç”¨FQEï¼ˆè§ç­–ç•¥è¯„ä¼°ç« èŠ‚ï¼‰

### æŸå¤±å‡½æ•°è®¡ç®—

```python
def compute_cql_loss(agent, batch, config):
    states, actions, rewards, next_states, dones = batch
    states = states.to(DEVICE)
    actions = actions.unsqueeze(1).to(DEVICE)  # [batch, 1]
    rewards = rewards.unsqueeze(1).to(DEVICE)
    next_states = next_states.to(DEVICE)
    dones = dones.unsqueeze(1).to(DEVICE)
    
    # 1. Bellmanè¯¯å·®
    q1_data = agent.q1(states, actions)
    q2_data = agent.q2(states, actions)
    
    with torch.no_grad():
        next_actions, next_logp, _ = agent.policy.sample(next_states)
        next_q1_target = agent.q1_target(next_states, next_actions)
        next_q2_target = agent.q2_target(next_states, next_actions)
        next_q_target = torch.min(next_q1_target, next_q2_target) - next_logp
        backup = rewards + config['gamma'] * (1 - dones) * next_q_target
    
    # ä½¿ç”¨HuberæŸå¤±ï¼ˆå¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼‰
    q1_loss = F.huber_loss(q1_data, backup, delta=1.0)
    q2_loss = F.huber_loss(q2_data, backup, delta=1.0)
    
    # 2. CQLæ­£åˆ™é¡¹ï¼ˆè¿ç»­åŠ¨ä½œç©ºé—´ï¼‰
    batch_size = states.shape[0]
    cql_samples = config.get('cql_samples', 10)
    
    # éšæœºåŠ¨ä½œï¼ˆåœ¨[-1, 1]èŒƒå›´å†…ï¼‰
    random_actions = torch.empty(batch_size, cql_samples, action_dim, device=DEVICE).uniform_(-1, 1)
    
    # ç­–ç•¥åŠ¨ä½œ
    policy_actions, _, _ = agent.policy.sample(states)
    
    # æ‰©å±•çŠ¶æ€å¹¶è®¡ç®—Qå€¼
    states_rep = states.unsqueeze(1).expand(-1, cql_samples, -1).reshape(-1, states.shape[-1])
    random_actions_flat = random_actions.reshape(-1, action_dim)
    q1_rand = agent.q1(states_rep, random_actions_flat).reshape(batch_size, cql_samples, 1)
    q2_rand = agent.q2(states_rep, random_actions_flat).reshape(batch_size, cql_samples, 1)
    
    # ç­–ç•¥åŠ¨ä½œçš„Qå€¼
    q1_policy = agent.q1(states, policy_actions)
    q2_policy = agent.q2(states, policy_actions)
    
    # logsumexp
    q1_cat = torch.cat([q1_rand, q1_policy.unsqueeze(1)], dim=1)
    q2_cat = torch.cat([q2_rand, q2_policy.unsqueeze(1)], dim=1)
    cql1 = torch.logsumexp(q1_cat, dim=1).mean() - q1_data.mean()
    cql2 = torch.logsumexp(q2_cat, dim=1).mean() - q2_data.mean()
    
    # 3. ç­–ç•¥æŸå¤±ï¼ˆè¡Œä¸ºå…‹éš†ï¼‰
    policy_dist = torch.distributions.Normal(*agent.policy(states))
    policy_log_prob = policy_dist.log_prob(actions.squeeze(1)).sum(dim=-1, keepdim=True)
    policy_loss = -policy_log_prob.mean()
    
    # æ€»æŸå¤±
    total_loss = q1_loss + q2_loss + config['alpha'] * (cql1 + cql2) + 0.1 * policy_loss
    
    return total_loss, info
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨HuberæŸå¤±ä»£æ›¿MSEï¼Œå¯¹å¼‚å¸¸å¥–åŠ±æ›´é²æ£’
- CQLæ­£åˆ™é¡¹ä½¿ç”¨logsumexpï¼Œå¯¹é‡‡æ ·åŠ¨ä½œï¼ˆéšæœº+ç­–ç•¥ï¼‰çš„Qå€¼å–logsumexp
- Alphaå‚æ•°è®¾ç½®ä¸º0.01ï¼Œé¿å…Qå€¼è¢«è¿‡åº¦æ‹‰ä½

### è®­ç»ƒæ­¥éª¤

1. **Qç½‘ç»œåˆå§‹åŒ–**ï¼šé‡æ–°åˆå§‹åŒ–ä¸ºæ¥è¿‘0çš„å°å€¼ï¼ˆè§æ¨¡å‹å®ç°ç« èŠ‚ï¼‰
   - è¾“å‡ºå±‚ï¼šuniform(-0.01, 0.01)
   - éšè—å±‚ï¼šXavier uniform (gain=0.5)
   - éªŒè¯åˆå§‹åŒ–åQå€¼èŒƒå›´åº”è¯¥åœ¨[-0.01, 0.01]é™„è¿‘

2. **é‡‡æ ·batch**ï¼šä»DataLoaderä¸­é‡‡æ ·256ä¸ªè½¬ç§»

3. **å‰å‘ä¼ æ’­**ï¼š
   - Qç½‘ç»œï¼šQ(s, a)
   - ç­–ç•¥ç½‘ç»œï¼šÏ€(a|s)
   - ç›®æ ‡Qç½‘ç»œï¼šQ_target(s', a')

4. **è®¡ç®—æŸå¤±**ï¼šBellmanè¯¯å·® + CQLæ­£åˆ™é¡¹ + ç­–ç•¥æŸå¤±
   - Bellmanè¯¯å·®ï¼šHuberæŸå¤±ï¼ˆdelta=1.0ï¼‰
   - CQLæ­£åˆ™é¡¹ï¼šlogsumexp(Q_samples) - Q_data
   - ç­–ç•¥æŸå¤±ï¼š-log Ï€(a_data|s)

5. **åå‘ä¼ æ’­**ï¼šè®¡ç®—æ¢¯åº¦

6. **æ¢¯åº¦è£å‰ª**ï¼šmax_norm=5.0ï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰

7. **æ›´æ–°å‚æ•°**ï¼šoptimizer.step()

8. **è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ**ï¼šÏ„=0.005

9. **éªŒè¯**ï¼šæ¯2ä¸ªepochåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ï¼ˆç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
   - æ³¨æ„ï¼šéªŒè¯æŸå¤±åªæ˜¯è®­ç»ƒæŒ‡æ ‡ï¼Œä¸æ˜¯ç­–ç•¥è¯„ä¼°

10. **æ—©åœ**ï¼šéªŒè¯æŸå¤±è¿ç»­10æ¬¡ä¸ä¸‹é™åˆ™åœæ­¢

### è®­ç»ƒç›‘æ§

**å…³é”®æŒ‡æ ‡**ï¼š
- **è®­ç»ƒæŸå¤±**ï¼šåº”è¯¥é€æ¸ä¸‹é™
- **Qå€¼å‡å€¼**ï¼šåº”è¯¥ä»æ¥è¿‘0é€æ¸ä¸Šå‡ï¼Œæœ€ç»ˆæ¥è¿‘å¥–åŠ±èŒƒå›´
- **éªŒè¯æŸå¤±**ï¼šå…ˆé™åå‡è¡¨ç¤ºè¿‡æ‹Ÿåˆï¼ˆæ­£å¸¸ç°è±¡ï¼‰

**æ­£å¸¸è®­ç»ƒè¿‡ç¨‹**ï¼š
```
Epoch   1 | è®­ç»ƒæŸå¤±: 10.0000 | Q1å‡å€¼: -0.05
Epoch   5 | è®­ç»ƒæŸå¤±: 5.0000  | Q1å‡å€¼: 0.50
Epoch  10 | è®­ç»ƒæŸå¤±: 2.0000  | Q1å‡å€¼: 2.00
Epoch  20 | è®­ç»ƒæŸå¤±: 1.0000  | Q1å‡å€¼: 5.00
```

**å¼‚å¸¸æƒ…å†µ**ï¼š
- Qå€¼ä¸€ç›´å¾ˆä½ï¼ˆ< -50ï¼‰ï¼šæ¨¡å‹æœªè®­ç»ƒå¥½æˆ–åˆå§‹åŒ–æœ‰é—®é¢˜
- è®­ç»ƒæŸå¤±ä¸ä¸‹é™ï¼šå­¦ä¹ ç‡å¯èƒ½å¤ªå°æˆ–ç½‘ç»œç»“æ„é—®é¢˜
- éªŒè¯æŸå¤±ä¸€ç›´ä¸Šå‡ï¼šå¯èƒ½è¿‡æ‹Ÿåˆï¼Œä½†è¿™æ˜¯æ­£å¸¸çš„ï¼ˆæ—©åœä¼šå¤„ç†ï¼‰

**é‡è¦è¯´æ˜**ï¼š
- **éªŒè¯é›†çš„ä½œç”¨**ï¼šéªŒè¯é›†ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé€‰æ‹©æœ€ä½³æ¨¡å‹ï¼Œä¸æ˜¯è¯„ä¼°ç­–ç•¥å¥½å
- **éªŒè¯æŸå¤±**ï¼šåªæ˜¯è®­ç»ƒè¿‡ç¨‹çš„ç›‘æ§æŒ‡æ ‡ï¼Œä¸èƒ½ç”¨äºè¯„ä¼°ç­–ç•¥æ€§èƒ½
- **ç­–ç•¥è¯„ä¼°**ï¼šè®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨FQEæ–¹æ³•è¯„ä¼°ç­–ç•¥ä»·å€¼ï¼ˆè§ç­–ç•¥è¯„ä¼°ç« èŠ‚ï¼‰

---

## ç­–ç•¥è¯„ä¼°

### é‡è¦è¯´æ˜

**éªŒè¯é›† vs ç­–ç•¥è¯„ä¼°**ï¼š
- âŒ **éªŒè¯é›†**ï¼šç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä¸æ˜¯è¯„ä¼°ç­–ç•¥å¥½å
  - éªŒè¯é›†çš„åŠ¨ä½œä¸ä¸€å®šæ˜¯æœ€ä¼˜çš„
  - éªŒè¯æŸå¤±åªæ˜¯è®­ç»ƒè¿‡ç¨‹çš„ç›‘æ§æŒ‡æ ‡
- âœ… **ç­–ç•¥è¯„ä¼°**ï¼šä½¿ç”¨FQEæ–¹æ³•ï¼Œè¯„ä¼°ç­–ç•¥çš„çœŸå®è¡¨ç°
  - ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼ˆä¸åˆ’åˆ†è®­ç»ƒ/éªŒè¯ï¼‰
  - è¯„ä¼°ç­–ç•¥ä»·å€¼ï¼ˆæœŸæœ›ç´¯ç§¯å¥–åŠ±ï¼‰

### FQE (Fitted Q Evaluation)

**åŸç†**ï¼š
```
V^Ï€(s) = E_a~Ï€[Q^Ï€(s, a)]

å¯¹äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œä½¿ç”¨è’™ç‰¹å¡æ´›ä¼°è®¡ï¼š
V^Ï€(s) â‰ˆ (1/N) * Î£_i Q(s, a_i),  a_i ~ Ï€(Â·|s)
```

**ä¸ºä»€ä¹ˆä½¿ç”¨FQE**ï¼š
- å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼Œä¸æ˜¯é¢„æµ‹"æ­£ç¡®"çš„åŠ¨ä½œ
- å‡†ç¡®ç‡æ˜¯åˆ†ç±»é—®é¢˜çš„æŒ‡æ ‡ï¼Œä¸é€‚åˆå¼ºåŒ–å­¦ä¹ 
- FQEä½¿ç”¨Qå‡½æ•°è¯„ä¼°ç­–ç•¥çš„æœŸæœ›è¡¨ç°ï¼Œæ˜¯ç¦»çº¿RLçš„æ ‡å‡†æ–¹æ³•

**å®ç°**ï¼š
```python
def evaluate_policy_value_fqe(agent, states, rewards, dones, gamma=0.99, n_samples=10):
    """
    ä½¿ç”¨Fitted Q Evaluation (FQE) è¯„ä¼°ç­–ç•¥ä»·å€¼ï¼ˆè¿ç»­åŠ¨ä½œç©ºé—´ï¼‰
    
    å¯¹äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œä½¿ç”¨è’™ç‰¹å¡æ´›ä¼°è®¡ï¼š
    V^Ï€(s) â‰ˆ (1/N) * Î£_i Q(s, a_i),  a_i ~ Ï€(Â·|s)
    """
    agent.eval()
    states_tensor = torch.FloatTensor(states).to(device)
    
    with torch.no_grad():
        # å¤šæ¬¡é‡‡æ ·å–å¹³å‡ï¼ˆè’™ç‰¹å¡æ´›ä¼°è®¡ï¼‰
        policy_values_list = []
        
        for _ in range(n_samples):
            policy_actions, _, _ = agent.policy.sample(states_tensor)
            q1_vals = agent.q1(states_tensor, policy_actions)
            q2_vals = agent.q2(states_tensor, policy_actions)
            q_vals = (q1_vals + q2_vals) / 2
            policy_values_list.append(q_vals.cpu().numpy())
        
        # å¹³å‡Qå€¼ï¼ˆç­–ç•¥ä»·å€¼ï¼‰
        policy_values = np.mean(policy_values_list, axis=0).flatten()
    
    # æŒ‰episodeåˆ†ç»„ï¼Œè®¡ç®—æ¯ä¸ªepisodeçš„åˆå§‹çŠ¶æ€ä»·å€¼
    episode_initial_values = []
    episode_actual_returns = []
    
    current_return = 0.0
    current_discount = 1.0
    episode_start_idx = 0
    
    for i in range(len(dones)):
        current_return += current_discount * rewards[i]
        current_discount *= gamma
        
        if dones[i] == 1 or i == len(dones) - 1:
            if episode_start_idx < len(policy_values):
                episode_initial_values.append(policy_values[episode_start_idx])
                episode_actual_returns.append(current_return)
            current_return = 0.0
            current_discount = 1.0
            episode_start_idx = i + 1
    
    return {
        'mean_episode_value': np.mean(episode_initial_values),
        'episode_initial_values': episode_initial_values,
        'episode_actual_returns': episode_actual_returns,
    }
```

**ä½¿ç”¨å…¨éƒ¨æ•°æ®**ï¼šè¯„ä¼°æ—¶ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œä¸åˆ’åˆ†è®­ç»ƒ/éªŒè¯ï¼Œå› ä¸ºè¯„ä¼°çš„æ˜¯ç­–ç•¥ä»·å€¼ï¼Œä¸æ˜¯é¢„æµ‹å‡†ç¡®ç‡ã€‚

### è¯„ä¼°æŒ‡æ ‡

1. **ç­–ç•¥ä»·å€¼ï¼ˆFQEï¼‰**ï¼šæœ€é‡è¦çš„æŒ‡æ ‡
   - ä½¿ç”¨Qå‡½æ•°è¯„ä¼°ç­–ç•¥çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±
   - ä¸è¡Œä¸ºç­–ç•¥å¯¹æ¯”ï¼Œçœ‹æ˜¯å¦æ”¹è¿›

2. **è¡Œä¸ºç­–ç•¥ä»·å€¼**ï¼šæ•°æ®ä¸­çš„å®é™…å›æŠ¥
   - ä½¿ç”¨å®é™…è½¨è¿¹çš„ç´¯ç§¯å¥–åŠ±

3. **ç­–ç•¥æ”¹è¿›**ï¼š
   - ç»å¯¹æ”¹è¿›ï¼šCQLç­–ç•¥ä»·å€¼ - è¡Œä¸ºç­–ç•¥ä»·å€¼
   - ç›¸å¯¹æ”¹è¿›ï¼š(ç»å¯¹æ”¹è¿› / è¡Œä¸ºç­–ç•¥ä»·å€¼) Ã— 100%

### è¯„ä¼°ç»“æœè§£è¯»

- **ç­–ç•¥ä»·å€¼ > è¡Œä¸ºç­–ç•¥ä»·å€¼**ï¼šç­–ç•¥æ›´å¥½
- **ç­–ç•¥ä»·å€¼ < è¡Œä¸ºç­–ç•¥ä»·å€¼**ï¼šç­–ç•¥æ›´ä¿å®ˆï¼ˆåœ¨åŒ»ç–—ä¸­å¯èƒ½æ˜¯ä¼˜ç‚¹ï¼‰
- **ç­–ç•¥ä»·å€¼ â‰ˆ è¡Œä¸ºç­–ç•¥ä»·å€¼**ï¼šç­–ç•¥ä¸è¡Œä¸ºç­–ç•¥ç›¸å½“

**æ­£å¸¸èŒƒå›´**ï¼š
- ç­–ç•¥ä»·å€¼ï¼š10-30ï¼ˆå½’ä¸€åŒ–åï¼‰
- è¡Œä¸ºç­–ç•¥å›æŠ¥ï¼š10-30ï¼ˆå½’ä¸€åŒ–åï¼‰
- ç­–ç•¥æ”¹è¿›ï¼š-20% åˆ° +20%

**å¼‚å¸¸æƒ…å†µ**ï¼š
- å¦‚æœç­–ç•¥ä»·å€¼ < -50ï¼Œè¯´æ˜æ¨¡å‹å¯èƒ½æœªè®­ç»ƒå¥½æˆ–åˆå§‹åŒ–æœ‰é—®é¢˜
- è¿è¡Œè¯Šæ–­è„šæœ¬ï¼š`python diagnose_model_issues.py`

---

## æ¨ç†æ¥å£

### é¢„æµ‹å‡½æ•°

æ ¹æ®æ‚£è€…çŠ¶æ€é¢„æµ‹åŠ¨ä½œï¼ˆè¯ç‰©å‰‚é‡ï¼‰çš„å‡½æ•°ï¼š

```python
def predict_action(agent, patient_state_raw, state_scaler, action_scaler, device=DEVICE):
    """
    æ ¹æ®æ‚£è€…çŠ¶æ€é¢„æµ‹åŠ¨ä½œï¼ˆè¯ç‰©å‰‚é‡ï¼‰
    
    å‚æ•°:
        agent: è®­ç»ƒå¥½çš„CQL agent
        patient_state_raw: æ‚£è€…çŠ¶æ€ï¼ˆåŸå§‹å€¼ï¼Œæœªæ ‡å‡†åŒ–ï¼‰ï¼Œ7ç»´æ•°ç»„
        state_scaler: çŠ¶æ€æ ‡å‡†åŒ–å™¨
        action_scaler: åŠ¨ä½œæ ‡å‡†åŒ–å™¨
        device: è®¡ç®—è®¾å¤‡
    
    è¿”å›:
        action: é¢„æµ‹çš„åŠ¨ä½œï¼ˆåŸå§‹å€¼ï¼Œmgï¼‰
        action_mean_norm: ç­–ç•¥å‡å€¼ï¼ˆæ ‡å‡†åŒ–ï¼‰
        action_std_norm: ç­–ç•¥æ ‡å‡†å·®ï¼ˆæ ‡å‡†åŒ–ï¼‰
        q_value: Qå€¼ä¼°è®¡
    """
    agent.eval()
    
    # 1. æ ‡å‡†åŒ–çŠ¶æ€
    patient_state = state_scaler.transform([patient_state_raw])
    patient_state_tensor = torch.FloatTensor(patient_state).to(device)
    
    # 2. è·å–ç­–ç•¥åŠ¨ä½œ
    with torch.no_grad():
        action_mean, action_std = agent.policy(patient_state_tensor)
        action_mean_norm = torch.tanh(action_mean)  # é™åˆ¶åˆ°[-1, 1]
        
        # 3. è®¡ç®—Qå€¼
        q1_val = agent.q1(patient_state_tensor, action_mean_norm)
        q2_val = agent.q2(patient_state_tensor, action_mean_norm)
        q_value = (q1_val + q2_val) / 2
        
        # 4. åæ ‡å‡†åŒ–åŠ¨ä½œï¼ˆä»[-1, 1]æ¢å¤åˆ°åŸå§‹èŒƒå›´ï¼‰
        action_mean_np = action_mean_norm.cpu().numpy().reshape(1, -1)
        action_raw = action_scaler.inverse_transform(action_mean_np)[0][0]
        
        # 5. ç¡®ä¿åŠ¨ä½œéè´Ÿï¼ˆè¯ç‰©å‰‚é‡ä¸èƒ½ä¸ºè´Ÿï¼‰
        action_raw = max(0, action_raw)
    
    return {
        'action': action_raw,  # åŸå§‹å€¼ï¼ˆmgï¼‰
        'action_mean_norm': action_mean_norm.cpu().numpy()[0, 0],
        'action_std_norm': action_std.cpu().numpy()[0, 0],
        'q_value': q_value.cpu().numpy()[0, 0],
    }
```

**æ³¨æ„**ï¼šæ­¤å‡½æ•°éœ€è¦åœ¨ä½¿ç”¨æ—¶å®ç°ï¼Œæˆ–ä»è¯„ä¼°è„šæœ¬ä¸­æå–ã€‚

### ä½¿ç”¨ç¤ºä¾‹

```python
import torch
import numpy as np
from cql_continuous_train import CQLAgent

# åŠ è½½æ¨¡å‹ï¼ˆPyTorch 2.6+éœ€è¦è®¾ç½®weights_only=Falseï¼‰
checkpoint = torch.load('cql_final_model.pt', weights_only=False)
state_scaler = checkpoint['state_scaler']
action_scaler = checkpoint['action_scaler']
state_dim = checkpoint['state_dim']
action_dim = checkpoint['action_dim']

# åˆå§‹åŒ–æ¨¡å‹
agent = CQLAgent(state_dim, action_dim).to('cuda' if torch.cuda.is_available() else 'cpu')
agent.load_state_dict(checkpoint['agent'])
agent.eval()

# æ‚£è€…çŠ¶æ€ï¼ˆåŸå§‹å€¼ï¼ŒæŒ‰STATE_COLSé¡ºåºï¼‰
patient_state = np.array([
    12.0,    # vanco_level(ug/mL)
    1.2,     # creatinine(mg/dL)
    8.0,     # wbc(K/uL)
    20.0,    # bun(mg/dL)
    37.5,    # temperature
    120,     # sbp
    85,      # heart_rate
])

# æ–¹æ³•1ï¼šä½¿ç”¨predict_actionå‡½æ•°ï¼ˆå¦‚æœå·²å®ç°ï¼‰
# result = predict_action(agent, patient_state, state_scaler, action_scaler)
# print(f"æ¨èå‰‚é‡: {result['action']:.1f} mg")
# print(f"Qå€¼: {result['q_value']:.4f}")

# æ–¹æ³•2ï¼šç›´æ¥ä½¿ç”¨ç­–ç•¥ç½‘ç»œ
with torch.no_grad():
    # æ ‡å‡†åŒ–çŠ¶æ€
    patient_state_scaled = state_scaler.transform([patient_state])
    patient_state_tensor = torch.FloatTensor(patient_state_scaled).to(agent.q1.net[0].weight.device)
    
    # è·å–ç­–ç•¥åŠ¨ä½œï¼ˆç¡®å®šæ€§åŠ¨ä½œï¼‰
    action_mean, action_std = agent.policy(patient_state_tensor)
    action_mean_norm = torch.tanh(action_mean)  # é™åˆ¶åˆ°[-1, 1]
    
    # è®¡ç®—Qå€¼
    q1_val = agent.q1(patient_state_tensor, action_mean_norm)
    q2_val = agent.q2(patient_state_tensor, action_mean_norm)
    q_value = (q1_val + q2_val) / 2
    
    # åæ ‡å‡†åŒ–åŠ¨ä½œ
    action_raw = action_scaler.inverse_transform(action_mean_norm.cpu().numpy())[0][0]
    action_raw = max(0, action_raw)  # ç¡®ä¿éè´Ÿ
    
    print(f"æ¨èå‰‚é‡: {action_raw:.1f} mg")
    print(f"Qå€¼: {q_value.item():.4f}")
    print(f"ç­–ç•¥æ ‡å‡†å·®: {action_std.item():.4f}")
```

---

## ä»£ç ç»“æ„

### è®­ç»ƒè„šæœ¬ (`cql_continuous_train.py`)

```python
# 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
#    - åŠ è½½CSVæ•°æ®
#    - æŒ‰stay_idåˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
#    - æ ‡å‡†åŒ–çŠ¶æ€å’ŒåŠ¨ä½œ
#    - å½’ä¸€åŒ–å¥–åŠ±

# 2. æ„å»ºæ•°æ®é›†
#    - RLDatasetç±»
#    - DataLoader

# 3. æ¨¡å‹å®šä¹‰
#    - GaussianPolicy: é«˜æ–¯ç­–ç•¥
#    - QNetwork: Qç½‘ç»œ
#    - CQLAgent: CQL Agent

# 4. Qç½‘ç»œåˆå§‹åŒ–ä¿®å¤ï¼ˆé‡è¦ï¼‰
#    - é‡æ–°åˆå§‹åŒ–ä¸ºæ¥è¿‘0çš„å°å€¼
#    - éªŒè¯åˆå§‹åŒ–åQå€¼èŒƒå›´

# 5. CQLæŸå¤±å‡½æ•°
#    - Bellmanè¯¯å·®ï¼ˆHuberæŸå¤±ï¼‰
#    - CQLæ­£åˆ™é¡¹ï¼ˆlogsumexpï¼‰
#    - ç­–ç•¥æŸå¤±

# 6. è®­ç»ƒé…ç½®
#    - alpha=0.01ï¼ˆå·²é™ä½ï¼‰
#    - å…¶ä»–è¶…å‚æ•°

# 7. è®­ç»ƒå¾ªç¯
#    - è®­ç»ƒä¸€ä¸ªepoch
#    - éªŒè¯ï¼ˆæ¯2ä¸ªepochï¼‰
#    - æ—©åœæœºåˆ¶

# 8. ä¿å­˜æ¨¡å‹
#    - cql_best_model.ptï¼ˆæœ€ä½³éªŒè¯æ€§èƒ½ï¼‰
#    - cql_final_model.ptï¼ˆæœ€ç»ˆæ¨¡å‹ï¼‰
```

### è¯„ä¼°è„šæœ¬ (`evaluate_cql_continuous.py`)

```python
# 1. åŠ è½½æ¨¡å‹
#    - ä½¿ç”¨weights_only=Falseï¼ˆPyTorch 2.6+ï¼‰

# 2. åŠ è½½å…¨éƒ¨æ•°æ®
#    - ä¸åˆ’åˆ†è®­ç»ƒ/éªŒè¯
#    - æ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–

# 3. FQEè¯„ä¼°
#    - å¤šæ¬¡é‡‡æ ·å–å¹³å‡
#    - è®¡ç®—ç­–ç•¥ä»·å€¼

# 4. è¡Œä¸ºç­–ç•¥è¯„ä¼°
#    - è®¡ç®—å®é™…å›æŠ¥

# 5. ç­–ç•¥æ”¹è¿›åˆ†æ
#    - ç»å¯¹æ”¹è¿›
#    - ç›¸å¯¹æ”¹è¿›

# 6. Episodeçº§åˆ«åˆ†æ
#    - æ¯ä¸ªstay_idçš„è¯¦ç»†ç»Ÿè®¡

# 7. å¯è§†åŒ–
#    - 6ä¸ªå›¾è¡¨

# 8. ä¿å­˜ç»“æœ
#    - JSONæ–‡ä»¶
#    - PNGå›¾è¡¨
```

## ä½¿ç”¨æŒ‡å—

### 1. è®­ç»ƒæ¨¡å‹

**æ–¹å¼1ï¼šç›´æ¥è¿è¡ŒPythonè„šæœ¬**
```bash
python cql_continuous_train.py
```

**æ–¹å¼2ï¼šåœ¨Jupyter Notebookä¸­è¿è¡Œ**
- å°†ä»£ç æŒ‰cellåˆ†å‰²
- æŒ‰é¡ºåºæ‰§è¡Œæ¯ä¸ªcell

**è®­ç»ƒè¾“å‡ºæ£€æŸ¥**ï¼š
- åˆå§‹åŒ–åQå€¼èŒƒå›´åº”è¯¥åœ¨[-0.01, 0.01]é™„è¿‘
- è®­ç»ƒè¿‡ç¨‹ä¸­Qå€¼åº”è¯¥é€æ¸ä¸Šå‡
- è®­ç»ƒæŸå¤±åº”è¯¥é€æ¸ä¸‹é™

### 2. æ¨¡å‹æ–‡ä»¶

è®­ç»ƒå®Œæˆåä¼šç”Ÿæˆï¼š
- `cql_best_model.pt`ï¼šæœ€ä½³éªŒè¯æ€§èƒ½çš„æ¨¡å‹ï¼ˆç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
- `cql_final_model.pt`ï¼šæœ€ç»ˆæ¨¡å‹ï¼ˆåŒ…å«æ‰€æœ‰å…ƒæ•°æ®ï¼Œç”¨äºæ¨ç†å’Œè¯„ä¼°ï¼‰

**æ³¨æ„**ï¼šéªŒè¯æŸå¤±åªæ˜¯è®­ç»ƒæŒ‡æ ‡ï¼Œä¸èƒ½è¯„ä¼°ç­–ç•¥å¥½åã€‚çœŸæ­£çš„ç­–ç•¥è¯„ä¼°è§ä¸‹ä¸€æ­¥ã€‚

### 3. ç­–ç•¥è¯„ä¼°

**è¿è¡Œè¯„ä¼°è„šæœ¬**ï¼š
```bash
python evaluate_cql_continuous.py
```

**è¯„ä¼°å†…å®¹**ï¼š
- FQEç­–ç•¥ä»·å€¼è¯„ä¼°
- ä¸è¡Œä¸ºç­–ç•¥å¯¹æ¯”
- Episodeçº§åˆ«åˆ†æ
- å¯è§†åŒ–å›¾è¡¨

**å¦‚æœè¯„ä¼°ç»“æœå¼‚å¸¸**ï¼ˆå¦‚ç­–ç•¥ä»·å€¼-100ï¼‰ï¼Œè¿è¡Œè¯Šæ–­ï¼š
```bash
python diagnose_model_issues.py
```

### 4. åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†

```python
import torch
import numpy as np

# åŠ è½½æ¨¡å‹ï¼ˆPyTorch 2.6+éœ€è¦è®¾ç½®weights_only=Falseï¼‰
checkpoint = torch.load('cql_final_model.pt', weights_only=False)
state_scaler = checkpoint['state_scaler']
action_scaler = checkpoint['action_scaler']
STATE_COLS = checkpoint['STATE_COLS']

# åˆå§‹åŒ–æ¨¡å‹
from cql_continuous_train import CQLAgent
agent = CQLAgent(
    state_dim=checkpoint['state_dim'],
    action_dim=checkpoint['action_dim']
).to('cuda' if torch.cuda.is_available() else 'cpu')
agent.load_state_dict(checkpoint['agent'])
agent.eval()

# ä½¿ç”¨æ¨¡å‹
patient_state = np.array([12.0, 1.2, 8.0, 20.0, 37.5, 120, 85])
result = predict_action(agent, patient_state, state_scaler, action_scaler)
print(f"æ¨èå‰‚é‡: {result['action']:.1f} mg")
```

**æ³¨æ„**ï¼šPyTorch 2.6+ç‰ˆæœ¬éœ€è¦è®¾ç½®`weights_only=False`æ¥åŠ è½½åŒ…å«sklearnå¯¹è±¡ï¼ˆStandardScalerï¼‰çš„checkpointã€‚

---

## æŠ€æœ¯ç»†èŠ‚

### 1. åŠ¨ä½œæ ‡å‡†åŒ–

**é—®é¢˜**ï¼šåŠ¨ä½œèŒƒå›´å¯èƒ½å¾ˆå¤§ï¼ˆ0~3000 mgï¼‰ï¼Œç›´æ¥è®­ç»ƒä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨StandardScaleræ ‡å‡†åŒ–åŠ¨ä½œ
- è®­ç»ƒæ—¶ï¼šåŠ¨ä½œåœ¨[-1, 1]èŒƒå›´å†…
- æ¨ç†æ—¶ï¼šåæ ‡å‡†åŒ–å›åŸå§‹èŒƒå›´

### 2. CQLé‡‡æ ·ç­–ç•¥

**è¿ç»­åŠ¨ä½œç©ºé—´æ— æ³•æšä¸¾**ï¼š
- ç¦»æ•£åŠ¨ä½œï¼šå¯ä»¥æšä¸¾æ‰€æœ‰åŠ¨ä½œ
- è¿ç»­åŠ¨ä½œï¼šéœ€è¦é‡‡æ ·

**é‡‡æ ·æ–¹æ³•**ï¼š
- éšæœºåŠ¨ä½œï¼šåœ¨[-1, 1]èŒƒå›´å†…å‡åŒ€é‡‡æ ·
- ç­–ç•¥åŠ¨ä½œï¼šä»Ï€(Â·|s)é‡‡æ ·
- ç»„åˆï¼šlogsumexp(éšæœºåŠ¨ä½œQå€¼ + ç­–ç•¥åŠ¨ä½œQå€¼)

### 3. ç›®æ ‡ç½‘ç»œæ›´æ–°

**è½¯æ›´æ–°ï¼ˆSACé£æ ¼ï¼‰**ï¼š
```
Q_target â† Ï„ * Q + (1-Ï„) * Q_target,  Ï„=0.005
```

**ä¼˜ç‚¹**ï¼š
- ç›®æ ‡å€¼æ›´ç¨³å®š
- è®­ç»ƒæ›´å¹³æ»‘

### 4. æ¢¯åº¦è£å‰ª

```python
torch.nn.utils.clip_grad_norm_(agent.parameters(), 5.0)
```

**ä½œç”¨**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§

### 5. æ—©åœæœºåˆ¶

```python
patience = 10  # éªŒè¯æŸå¤±è¿ç»­10æ¬¡ä¸ä¸‹é™åˆ™åœæ­¢
```

**ä½œç”¨**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹

**æ³¨æ„**ï¼šéªŒè¯æŸå¤±åªæ˜¯è®­ç»ƒè¿‡ç¨‹çš„ç›‘æ§æŒ‡æ ‡ï¼Œä¸èƒ½ç”¨äºè¯„ä¼°ç­–ç•¥å¥½åã€‚çœŸæ­£çš„ç­–ç•¥è¯„ä¼°åº”è¯¥ä½¿ç”¨FQEæ–¹æ³•ã€‚

### 6. Qç½‘ç»œåˆå§‹åŒ–

**é—®é¢˜**ï¼šé»˜è®¤åˆå§‹åŒ–å¯èƒ½å¯¼è‡´Qå€¼è¿‡å¤§æˆ–è¿‡å°ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**ï¼š
- Qç½‘ç»œè¾“å‡ºå±‚åˆå§‹åŒ–ä¸ºæ¥è¿‘0çš„å°å€¼ï¼ˆ-0.01åˆ°0.01ï¼‰
- éšè—å±‚ä½¿ç”¨è¾ƒå°çš„Xavieråˆå§‹åŒ–ï¼ˆgain=0.5ï¼‰
- åˆå§‹åŒ–åéªŒè¯Qå€¼èŒƒå›´åº”è¯¥åœ¨[-0.01, 0.01]é™„è¿‘

**é‡è¦æ€§**ï¼šå¦‚æœQå€¼åˆå§‹åŒ–ä¸å½“ï¼Œå¯èƒ½å¯¼è‡´ï¼š
- Qå€¼è¿‡å¤§ï¼šCQLæ­£åˆ™é¡¹çˆ†ç‚¸ï¼Œè®­ç»ƒä¸ç¨³å®š
- Qå€¼è¿‡å°ï¼šç­–ç•¥ä»·å€¼å¼‚å¸¸ä½ï¼Œæ¨¡å‹æ— æ³•å­¦ä¹ 

---

## è¶…å‚æ•°è°ƒä¼˜å»ºè®®

### å­¦ä¹ ç‡ (lr)
- **é»˜è®¤**ï¼š1e-4
- **è°ƒä¼˜èŒƒå›´**ï¼š1e-5 ~ 3e-4
- **è¿‡å¤§**ï¼šè®­ç»ƒä¸ç¨³å®š
- **è¿‡å°**ï¼šè®­ç»ƒå¤ªæ…¢

### CQLæ­£åˆ™ç³»æ•° (alpha)
- **é»˜è®¤**ï¼š0.01ï¼ˆå·²ä»0.1é™ä½ï¼‰
- **è°ƒä¼˜èŒƒå›´**ï¼š0.001 ~ 0.1
- **è¿‡å¤§**ï¼šç­–ç•¥è¿‡äºä¿å®ˆï¼ŒQå€¼è¢«æ‹‰ä½
- **è¿‡å°**ï¼šCQLæ•ˆæœä¸æ˜æ˜¾
- **å½“å‰è®¾ç½®**ï¼š0.01ï¼Œå¹³è¡¡ä¿å®ˆæ€§å’Œè®­ç»ƒç¨³å®šæ€§

### æŠ˜æ‰£å› å­ (gamma)
- **é»˜è®¤**ï¼š0.99
- **åŒ»ç–—åœºæ™¯**ï¼šå¯ä»¥é™ä½åˆ°0.95ï¼ˆæ›´å…³æ³¨çŸ­æœŸï¼‰
- **é•¿æœŸä»»åŠ¡**ï¼šä¿æŒ0.99

### CQLé‡‡æ ·æ•° (cql_samples)
- **é»˜è®¤**ï¼š10
- **è°ƒä¼˜èŒƒå›´**ï¼š5 ~ 20
- **æ›´å¤šé‡‡æ ·**ï¼šæ›´å‡†ç¡®ä½†è®¡ç®—æ›´æ…¢

---

## è®­ç»ƒé—®é¢˜è¯Šæ–­

### Qå€¼å¼‚å¸¸é—®é¢˜

**ç—‡çŠ¶**ï¼š
- Qå€¼å‡å€¼ < -50
- ç­–ç•¥ä»·å€¼å¼‚å¸¸ä½ï¼ˆå¦‚-100ï¼‰
- è®­ç»ƒæŸå¤±ä¸ä¸‹é™

**å¯èƒ½åŸå› **ï¼š
1. Qç½‘ç»œåˆå§‹åŒ–é—®é¢˜ï¼šé»˜è®¤åˆå§‹åŒ–å¯¼è‡´Qå€¼è¿‡å¤§æˆ–è¿‡å°
2. Alphaå‚æ•°è¿‡å¤§ï¼šCQLæ­£åˆ™é¡¹å°†Qå€¼æ‹‰ä½
3. è®­ç»ƒä¸å……åˆ†ï¼šepochsä¸å¤Ÿæˆ–å­¦ä¹ ç‡ä¸åˆé€‚

**è¯Šæ–­æ–¹æ³•**ï¼š
```bash
python diagnose_model_issues.py
```

**ä¿®å¤æ–¹æ³•**ï¼š
1. é‡æ–°åˆå§‹åŒ–Qç½‘ç»œä¸ºå°å€¼ï¼ˆä»£ç ä¸­å·²å®ç°ï¼‰
2. é™ä½alphaå‚æ•°ï¼ˆå·²è®¾ç½®ä¸º0.01ï¼‰
3. æ£€æŸ¥è®­ç»ƒæ—¥å¿—ï¼Œç¡®ä¿Qå€¼é€æ¸ä¸Šå‡

### éªŒè¯æŸå¤± vs ç­–ç•¥è¯„ä¼°

**å¸¸è§è¯¯è§£**ï¼š
- âŒ éªŒè¯æŸå¤±ä½ = ç­–ç•¥å¥½
- âŒ éªŒè¯é›†ç”¨äºè¯„ä¼°ç­–ç•¥

**æ­£ç¡®ç†è§£**ï¼š
- âœ… éªŒè¯æŸå¤±åªæ˜¯è®­ç»ƒè¿‡ç¨‹çš„ç›‘æ§æŒ‡æ ‡
- âœ… éªŒè¯é›†ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä¸æ˜¯è¯„ä¼°ç­–ç•¥å¥½å
- âœ… çœŸæ­£çš„ç­–ç•¥è¯„ä¼°ä½¿ç”¨FQEæ–¹æ³•

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆä½¿ç”¨è¿ç»­åŠ¨ä½œç©ºé—´è€Œä¸æ˜¯ç¦»æ•£ï¼Ÿ

**ç­”**ï¼šè¯ç‰©å‰‚é‡æ˜¯è¿ç»­å€¼ï¼Œç¦»æ•£åŒ–ä¼šä¸¢å¤±ç²¾åº¦ã€‚è¿ç»­åŠ¨ä½œç©ºé—´å¯ä»¥ç»™å‡ºç²¾ç¡®çš„å‰‚é‡å»ºè®®ã€‚

### Q2: å¦‚ä½•ç¡®ä¿åŠ¨ä½œéè´Ÿï¼Ÿ

**ç­”**ï¼šåœ¨æ¨ç†æ—¶ï¼Œä½¿ç”¨`max(0, action_raw)`ç¡®ä¿åŠ¨ä½œéè´Ÿã€‚è®­ç»ƒæ—¶å…è®¸è´Ÿå€¼ï¼ˆæ ‡å‡†åŒ–åï¼‰ï¼Œä½†æ¨ç†æ—¶ä¼šä¿®æ­£ã€‚

### Q3: ç­–ç•¥è¯„ä¼°ä¸ºä»€ä¹ˆä¸ç”¨å‡†ç¡®ç‡ï¼Ÿ

**ç­”**ï¼šå¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼Œä¸æ˜¯é¢„æµ‹"æ­£ç¡®"çš„åŠ¨ä½œã€‚åº”è¯¥ä½¿ç”¨ç­–ç•¥ä»·å€¼ï¼ˆFQEï¼‰è¯„ä¼°ã€‚

### Q4: å¦‚ä½•è§£é‡Šç­–ç•¥æ›´ä¿å®ˆï¼Ÿ

**ç­”**ï¼šå¦‚æœCQLç­–ç•¥ä»·å€¼ < è¡Œä¸ºç­–ç•¥ä»·å€¼ï¼Œè¯´æ˜ç­–ç•¥æ›´ä¿å®ˆï¼ˆæ›´å€¾å‘äºä¸ç»™è¯æˆ–ç»™ä½å‰‚é‡ï¼‰ã€‚åœ¨åŒ»ç–—ä¸­ï¼Œä¿å®ˆå¯èƒ½æ˜¯ä¼˜ç‚¹ã€‚

### Q5: ä¸ºä»€ä¹ˆéªŒè¯é›†çš„åŠ¨ä½œä¸ä¸€å®šæ˜¯æœ€ä¼˜çš„ï¼Œè¿˜è¦åˆ’åˆ†éªŒè¯é›†ï¼Ÿ

**ç­”**ï¼šéªŒè¯é›†çš„ä½œç”¨æ˜¯é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä¸æ˜¯è¯„ä¼°ç­–ç•¥å¥½åã€‚
- éªŒè¯é›†çš„åŠ¨ä½œä¸ä¸€å®šæ˜¯æœ€ä¼˜çš„ï¼ˆä½ è¯´å¾—å¯¹ï¼ï¼‰
- éªŒè¯æŸå¤±åªæ˜¯è®­ç»ƒè¿‡ç¨‹çš„ç›‘æ§æŒ‡æ ‡ï¼Œç”¨äºé€‰æ‹©æœ€ä½³æ¨¡å‹
- çœŸæ­£çš„ç­–ç•¥è¯„ä¼°åº”è¯¥ä½¿ç”¨FQEæ–¹æ³•ï¼ˆç­–ç•¥ä»·å€¼ï¼‰

### Q6: PyTorch 2.6åŠ è½½æ¨¡å‹æŠ¥é”™æ€ä¹ˆåŠï¼Ÿ

**ç­”**ï¼šPyTorch 2.6é»˜è®¤`weights_only=True`ï¼Œæ— æ³•åŠ è½½åŒ…å«sklearnå¯¹è±¡çš„checkpointã€‚éœ€è¦è®¾ç½®`weights_only=False`ï¼š

```python
checkpoint = torch.load('cql_final_model.pt', weights_only=False)
```

### Q7: Qå€¼å¼‚å¸¸ä½ï¼ˆå¦‚-100ï¼‰æ€ä¹ˆåŠï¼Ÿ

**ç­”**ï¼šè¿™é€šå¸¸è¡¨ç¤ºæ¨¡å‹æœªè®­ç»ƒå¥½æˆ–åˆå§‹åŒ–æœ‰é—®é¢˜ã€‚æ£€æŸ¥ï¼š
1. Qç½‘ç»œæ˜¯å¦é‡æ–°åˆå§‹åŒ–ä¸ºå°å€¼
2. Alphaå‚æ•°æ˜¯å¦è¿‡å¤§ï¼ˆå»ºè®®0.01ï¼‰
3. è®­ç»ƒæ˜¯å¦å……åˆ†ï¼ˆè®­ç»ƒæŸå¤±æ˜¯å¦ä¸‹é™ï¼‰
4. è¿è¡Œè¯Šæ–­è„šæœ¬ï¼š`python diagnose_model_issues.py`

---

## æ€»ç»“

| æ–¹é¢ | è¯¦æƒ… |
|------|------|
| **ç®—æ³•** | Conservative Q-Learning (CQL) |
| **åŠ¨ä½œç©ºé—´** | è¿ç»­ï¼ˆè¯ç‰©å‰‚é‡ mgï¼‰ |
| **ç­–ç•¥ç±»å‹** | é«˜æ–¯ç­–ç•¥ï¼ˆGaussian Policyï¼‰ |
| **Qç½‘ç»œ** | Twin Qç½‘ç»œ |
| **è¯„ä¼°æ–¹æ³•** | FQE (Fitted Q Evaluation) |
| **è®­ç»ƒæ—¶é—´** | çº¦30-50 epochsï¼ˆå–å†³äºæ•°æ®é‡ï¼‰ |
| **æ¨¡å‹å¤§å°** | çº¦350Kå‚æ•° |
| **æ¨èç”¨é€”** | ICUè¯ç‰©å‰‚é‡å†³ç­–æ”¯æŒç³»ç»Ÿ |
| **å…³é”®ä¿®å¤** | Qç½‘ç»œåˆå§‹åŒ–ä¿®å¤ï¼Œalpha=0.01 |
| **éªŒè¯é›†ä½œç”¨** | é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä¸æ˜¯ç­–ç•¥è¯„ä¼° |

---

---

## é‡è¦æ›´æ–°è®°å½•

### v1.1 (2024)
- âœ… æ·»åŠ Qç½‘ç»œåˆå§‹åŒ–ä¿®å¤ï¼ˆé¿å…Qå€¼å¼‚å¸¸ï¼‰
- âœ… é™ä½alphaå‚æ•°ä»0.1åˆ°0.01
- âœ… æ˜ç¡®éªŒè¯é›†çš„ä½œç”¨ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä¸æ˜¯ç­–ç•¥è¯„ä¼°ï¼‰
- âœ… æ·»åŠ PyTorch 2.6+åŠ è½½æ¨¡å‹è¯´æ˜
- âœ… æ·»åŠ è¯Šæ–­è„šæœ¬ä½¿ç”¨è¯´æ˜

### v1.0 (2024)
- åˆå§‹ç‰ˆæœ¬

---

---

## ç›¸å…³æ–‡ä»¶

### æ ¸å¿ƒä»£ç æ–‡ä»¶
- `cql_continuous_train.py` - è®­ç»ƒè„šæœ¬ï¼ˆåŒ…å«Qç½‘ç»œåˆå§‹åŒ–ä¿®å¤ï¼‰
- `evaluate_cql_continuous.py` - ç­–ç•¥è¯„ä¼°è„šæœ¬ï¼ˆFQEæ–¹æ³•ï¼‰
- `diagnose_model_issues.py` - æ¨¡å‹è¯Šæ–­è„šæœ¬

### æ–‡æ¡£æ–‡ä»¶
- `CQL_è¿ç»­åŠ¨ä½œç©ºé—´_æŠ€æœ¯æ–‡æ¡£.md` - æœ¬æ–‡æ¡£
- `ç¦»çº¿RLä¸­éªŒè¯é›†çš„ä½œç”¨è¯´æ˜.md` - éªŒè¯é›†ä½œç”¨è¯¦ç»†è¯´æ˜
- `æ¨¡å‹è¯„ä¼°ä½¿ç”¨è¯´æ˜.md` - è¯„ä¼°æ–¹æ³•ä½¿ç”¨æŒ‡å—
- `ä¿®å¤Qå€¼å¼‚å¸¸_é‡æ–°è®­ç»ƒæŒ‡å—.md` - Qå€¼å¼‚å¸¸ä¿®å¤æŒ‡å—

### æ•°æ®æ–‡ä»¶
- `ready_data.csv` - è®­ç»ƒæ•°æ®

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.1  
**æœ€åæ›´æ–°**: 2024  
**ä½œè€…**: AI Assistant  
**å‚è€ƒ**: CQLåŸå§‹è®ºæ–‡, SACè®ºæ–‡  
**ä»£ç æ–‡ä»¶**: `cql_continuous_train.py`, `evaluate_cql_continuous.py`

